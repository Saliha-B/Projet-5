{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Deep Learning\n",
    "\n",
    "# Régression & Classification\n",
    "\n",
    "---\n",
    "\n",
    "# Partie 1 : Boston House Prices\n",
    "\n",
    "<img src=\"https://cdn10.bostonmagazine.com/wp-content/uploads/sites/2/2020/10/boston-foliage-social.jpg\" width=60%>\n",
    "\n",
    "Vous appliquerez vos nouvelles connaissances sur le **MLP (MultiLayer Perceptron)** à un cas d'utilisation de régression classique : la prévision du prix de l'immobilier sur l'ensemble de données **scikit-learn boston**.\n",
    "\n",
    "1. Chargez le jeu de données et préparez-le.\n",
    "2. Modélisez un réseau de neurones à cinq couches avec 100 neurones chacun et entraînez-le.\n",
    "3. Comparez vos résultats avec une régression linéaire classique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33613\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33613\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 251.9388 - mean_absolute_error: 11.4192 - val_loss: 73.7131 - val_mean_absolute_error: 6.4328\n",
      "Epoch 2/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 88.5186 - mean_absolute_error: 6.9317 - val_loss: 77.4360 - val_mean_absolute_error: 6.8790\n",
      "Epoch 3/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 99.5647 - mean_absolute_error: 7.4173 - val_loss: 79.3973 - val_mean_absolute_error: 6.1510\n",
      "Epoch 4/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 92.3869 - mean_absolute_error: 6.9564 - val_loss: 82.7812 - val_mean_absolute_error: 6.3111\n",
      "Epoch 5/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 103.3568 - mean_absolute_error: 7.3581 - val_loss: 234.5678 - val_mean_absolute_error: 14.0688\n",
      "Epoch 6/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 115.5015 - mean_absolute_error: 8.4523 - val_loss: 77.0042 - val_mean_absolute_error: 6.8331\n",
      "Epoch 7/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 98.6414 - mean_absolute_error: 7.3715 - val_loss: 76.0407 - val_mean_absolute_error: 6.7250\n",
      "Epoch 8/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 90.6951 - mean_absolute_error: 7.0359 - val_loss: 117.5280 - val_mean_absolute_error: 7.9334\n",
      "Epoch 9/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 112.9146 - mean_absolute_error: 7.6323 - val_loss: 224.6481 - val_mean_absolute_error: 13.7397\n",
      "Epoch 10/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 119.9594 - mean_absolute_error: 8.4013 - val_loss: 92.6520 - val_mean_absolute_error: 6.7881\n",
      "Epoch 11/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 104.4431 - mean_absolute_error: 7.2689 - val_loss: 74.0210 - val_mean_absolute_error: 5.9535\n",
      "Epoch 12/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 91.5107 - mean_absolute_error: 7.0026 - val_loss: 80.5481 - val_mean_absolute_error: 6.2068\n",
      "Epoch 13/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 102.8349 - mean_absolute_error: 7.4245 - val_loss: 94.8629 - val_mean_absolute_error: 6.8957\n",
      "Epoch 14/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 95.4883 - mean_absolute_error: 7.0441 - val_loss: 105.8147 - val_mean_absolute_error: 8.9457\n",
      "Epoch 15/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 97.8623 - mean_absolute_error: 7.3869 - val_loss: 71.3545 - val_mean_absolute_error: 6.0051\n",
      "Epoch 16/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 105.4321 - mean_absolute_error: 7.5498 - val_loss: 77.6973 - val_mean_absolute_error: 6.9104\n",
      "Epoch 17/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 93.5310 - mean_absolute_error: 7.1541 - val_loss: 77.2232 - val_mean_absolute_error: 6.0535\n",
      "Epoch 18/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 100.1114 - mean_absolute_error: 7.2774 - val_loss: 77.3729 - val_mean_absolute_error: 6.0595\n",
      "Epoch 19/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 89.2937 - mean_absolute_error: 6.8277 - val_loss: 73.7219 - val_mean_absolute_error: 5.9470\n",
      "Epoch 20/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 96.3067 - mean_absolute_error: 7.1553 - val_loss: 173.6807 - val_mean_absolute_error: 11.9072\n",
      "Epoch 21/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 117.8756 - mean_absolute_error: 8.5804 - val_loss: 71.3212 - val_mean_absolute_error: 6.0172\n",
      "Epoch 22/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 84.4808 - mean_absolute_error: 6.7537 - val_loss: 87.5262 - val_mean_absolute_error: 6.5446\n",
      "Epoch 23/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 94.6948 - mean_absolute_error: 6.7590 - val_loss: 82.4832 - val_mean_absolute_error: 6.2967\n",
      "Epoch 24/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 102.9935 - mean_absolute_error: 7.3664 - val_loss: 71.7806 - val_mean_absolute_error: 6.1526\n",
      "Epoch 25/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 86.3427 - mean_absolute_error: 6.7358 - val_loss: 72.5373 - val_mean_absolute_error: 5.9339\n",
      "Epoch 26/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 81.3419 - mean_absolute_error: 6.4801 - val_loss: 86.1388 - val_mean_absolute_error: 6.4772\n",
      "Epoch 27/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 95.4588 - mean_absolute_error: 6.9829 - val_loss: 82.4822 - val_mean_absolute_error: 7.3788\n",
      "Epoch 28/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 89.0449 - mean_absolute_error: 7.0281 - val_loss: 85.0639 - val_mean_absolute_error: 6.4229\n",
      "Epoch 29/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 101.5974 - mean_absolute_error: 7.2535 - val_loss: 110.7919 - val_mean_absolute_error: 9.2064\n",
      "Epoch 30/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 97.5832 - mean_absolute_error: 7.5393 - val_loss: 71.3924 - val_mean_absolute_error: 5.9619\n",
      "Epoch 31/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 97.3992 - mean_absolute_error: 7.1989 - val_loss: 74.7726 - val_mean_absolute_error: 5.9672\n",
      "Epoch 32/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 96.1599 - mean_absolute_error: 7.1246 - val_loss: 71.5897 - val_mean_absolute_error: 5.9416\n",
      "Epoch 33/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 103.6619 - mean_absolute_error: 7.4881 - val_loss: 74.8915 - val_mean_absolute_error: 5.9695\n",
      "Epoch 34/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 106.0712 - mean_absolute_error: 7.4129 - val_loss: 90.2096 - val_mean_absolute_error: 8.0049\n",
      "Epoch 35/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 89.0828 - mean_absolute_error: 7.2295 - val_loss: 71.5785 - val_mean_absolute_error: 6.1428\n",
      "Epoch 36/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 83.3023 - mean_absolute_error: 6.7573 - val_loss: 90.5770 - val_mean_absolute_error: 6.6883\n",
      "Epoch 37/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 109.4963 - mean_absolute_error: 7.5801 - val_loss: 241.3675 - val_mean_absolute_error: 14.2977\n",
      "Epoch 38/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 142.4197 - mean_absolute_error: 9.3291 - val_loss: 74.5482 - val_mean_absolute_error: 5.9561\n",
      "Epoch 39/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 91.8975 - mean_absolute_error: 6.9658 - val_loss: 72.8638 - val_mean_absolute_error: 5.9220\n",
      "Epoch 40/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 85.4395 - mean_absolute_error: 6.6190 - val_loss: 70.8614 - val_mean_absolute_error: 5.9852\n",
      "Epoch 41/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 92.4049 - mean_absolute_error: 7.1009 - val_loss: 70.7792 - val_mean_absolute_error: 6.0149\n",
      "Epoch 42/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 85.5316 - mean_absolute_error: 6.7358 - val_loss: 115.2464 - val_mean_absolute_error: 9.4377\n",
      "Epoch 43/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 99.0673 - mean_absolute_error: 7.5257 - val_loss: 73.8216 - val_mean_absolute_error: 6.5364\n",
      "Epoch 44/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 100.0844 - mean_absolute_error: 7.3924 - val_loss: 72.5595 - val_mean_absolute_error: 5.8838\n",
      "Epoch 45/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 86.4963 - mean_absolute_error: 6.6286 - val_loss: 70.3923 - val_mean_absolute_error: 5.8493\n",
      "Epoch 46/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 84.3248 - mean_absolute_error: 6.7783 - val_loss: 91.9036 - val_mean_absolute_error: 8.2024\n",
      "Epoch 47/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 86.1330 - mean_absolute_error: 7.0601 - val_loss: 103.4409 - val_mean_absolute_error: 8.8962\n",
      "Epoch 48/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 83.8237 - mean_absolute_error: 6.9445 - val_loss: 116.2006 - val_mean_absolute_error: 9.5869\n",
      "Epoch 49/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 78.3303 - mean_absolute_error: 6.5763 - val_loss: 206.4096 - val_mean_absolute_error: 13.3162\n",
      "Epoch 50/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 94.0747 - mean_absolute_error: 7.4964 - val_loss: 53.4202 - val_mean_absolute_error: 4.7900\n",
      "Epoch 51/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 63.1013 - mean_absolute_error: 5.3657 - val_loss: 60.2266 - val_mean_absolute_error: 5.3362\n",
      "Epoch 52/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 83.3167 - mean_absolute_error: 6.6267 - val_loss: 413.6741 - val_mean_absolute_error: 19.1991\n",
      "Epoch 53/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 136.9398 - mean_absolute_error: 9.2414 - val_loss: 74.4097 - val_mean_absolute_error: 6.2600\n",
      "Epoch 54/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 53.4399 - mean_absolute_error: 5.0582 - val_loss: 39.2040 - val_mean_absolute_error: 4.3906\n",
      "Epoch 55/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 38.3220 - mean_absolute_error: 4.3043 - val_loss: 33.6685 - val_mean_absolute_error: 4.1937\n",
      "Epoch 56/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 36.5174 - mean_absolute_error: 3.9342 - val_loss: 43.3758 - val_mean_absolute_error: 5.4985\n",
      "Epoch 57/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 40.9982 - mean_absolute_error: 4.5290 - val_loss: 24.7941 - val_mean_absolute_error: 3.4801\n",
      "Epoch 58/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 45.0464 - mean_absolute_error: 5.1583 - val_loss: 24.4820 - val_mean_absolute_error: 3.4398\n",
      "Epoch 59/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.8564 - mean_absolute_error: 3.5752 - val_loss: 23.3769 - val_mean_absolute_error: 3.3477\n",
      "Epoch 60/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.5293 - mean_absolute_error: 3.4954 - val_loss: 22.1682 - val_mean_absolute_error: 3.2733\n",
      "Epoch 61/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 39.4856 - mean_absolute_error: 4.6827 - val_loss: 33.9734 - val_mean_absolute_error: 4.4308\n",
      "Epoch 62/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 32.8076 - mean_absolute_error: 4.4504 - val_loss: 25.4389 - val_mean_absolute_error: 3.6654\n",
      "Epoch 63/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 31.1286 - mean_absolute_error: 4.2355 - val_loss: 23.7118 - val_mean_absolute_error: 3.5131\n",
      "Epoch 64/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25.8490 - mean_absolute_error: 3.9405 - val_loss: 105.9946 - val_mean_absolute_error: 8.8890\n",
      "Epoch 65/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 48.8942 - mean_absolute_error: 4.9406 - val_loss: 24.7290 - val_mean_absolute_error: 3.5335\n",
      "Epoch 66/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.7430 - mean_absolute_error: 3.3387 - val_loss: 25.5488 - val_mean_absolute_error: 3.6920\n",
      "Epoch 67/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.2584 - mean_absolute_error: 3.6347 - val_loss: 28.1535 - val_mean_absolute_error: 3.9287\n",
      "Epoch 68/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 18.7185 - mean_absolute_error: 3.2088 - val_loss: 38.9373 - val_mean_absolute_error: 4.8981\n",
      "Epoch 69/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.2207 - mean_absolute_error: 3.7081 - val_loss: 26.7057 - val_mean_absolute_error: 3.8063\n",
      "Epoch 70/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.3879 - mean_absolute_error: 3.9643 - val_loss: 50.1125 - val_mean_absolute_error: 6.0031\n",
      "Epoch 71/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 33.5094 - mean_absolute_error: 4.3671 - val_loss: 21.5055 - val_mean_absolute_error: 3.2758\n",
      "Epoch 72/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.2546 - mean_absolute_error: 3.2389 - val_loss: 28.8669 - val_mean_absolute_error: 4.1519\n",
      "Epoch 73/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.9605 - mean_absolute_error: 3.7283 - val_loss: 42.9257 - val_mean_absolute_error: 4.9811\n",
      "Epoch 74/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 34.9954 - mean_absolute_error: 4.5566 - val_loss: 59.1736 - val_mean_absolute_error: 6.2035\n",
      "Epoch 75/75\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 31.8166 - mean_absolute_error: 4.3153 - val_loss: 29.1527 - val_mean_absolute_error: 3.7284\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "r2_score = 0.5917029656581289\n",
      "r2_score = 0.6727073132687316\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 887\n",
      "[LightGBM] [Info] Number of data points in the train set: 354, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 22.759887\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "r2_score = 0.8387595000395716\n"
     ]
    }
   ],
   "source": [
    "# 1. Chargez le jeu de données et préparez-le.\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Split des donnees\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "\n",
    "# Standardiser les donnees\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2. Modélisez un réseau de neurones à cinq couches avec 100 neurones\n",
    "# chacun et entraînez-le.\n",
    "from sklearn.metrics import *\n",
    "# Definition de la fonction de création\n",
    "def model(input_dim):\n",
    "    # Nous créons un modèle dit séquentiel\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Ajouter le premier calque \"Dense\" de 3 unités, et donner la dimension d'entrée (ici 5)\n",
    "    model.add(tf.keras.layers.Dense(100, input_dim=input_dim, activation='sigmoid'))\n",
    "    \n",
    "    # Ajouter le deuxième calque \"Dense\" de 3 unités\n",
    "    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
    "\n",
    "    # Ajouter enfin la couche de sortie avec une unité: le résultat prédit\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "    # renvoie le modèle créé\n",
    "    return model  \n",
    "\n",
    "# Creation du modèle\n",
    "my_model = model(input_dim=X.shape[1])\n",
    "# Compilation :\n",
    "my_model.compile(optimizer=\"SGD\", loss=\"mean_squared_error\", \n",
    "                 metrics=[tf.keras.metrics.mae])\n",
    "\n",
    "my_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "            epochs=75, batch_size=None)\n",
    "\n",
    "y_pred = my_model.predict(X_test)\n",
    "print(\"r2_score =\", r2_score(y_test, y_pred))\n",
    "\n",
    "# 3. Comparez vos résultats avec une régression linéaire classique.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression(n_jobs=-1)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"r2_score =\", r2_score(y_test, y_pred))\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgbr = lgb.LGBMRegressor(n_jobs=-1)\n",
    "lgbr.fit(X_train, y_train)\n",
    "y_pred = lgbr.predict(X_test)\n",
    "print(\"r2_score =\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 2 : Est-ce qu'il pleut demain ?\n",
    "\n",
    "<img src=\"https://cdn.dnaindia.com/sites/default/files/styles/full/public/2019/01/18/779754-rain-room.jpg\" width=60%>\n",
    "\n",
    "Lignes directrices\n",
    "\n",
    "\n",
    "> Vous pouvez télécharger le jeu de données [ici](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) et le stocker dans votre dossier **`data`**.\n",
    "\n",
    "Dans cet exercice, nous allons essayer d'utiliser un réseau de neurones sur une tâche de prédiction typique : prédire si demain sera un jour de pluie ou non.\n",
    "\n",
    "1. Chargez le jeu de données et explorez-le. \n",
    "2. La valeur cible est la colonne **`RainTomorrow`**. Modélisez un réseau de neurones à cinq couches avec 100 neurones chacun et entraînez-le.\n",
    "3. Comparez les performances avec une régression logistique simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 145460 entries, 0 to 145459\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   Date           145460 non-null  object \n",
      " 1   Location       145460 non-null  object \n",
      " 2   MinTemp        143975 non-null  float64\n",
      " 3   MaxTemp        144199 non-null  float64\n",
      " 4   Rainfall       142199 non-null  float64\n",
      " 5   Evaporation    82670 non-null   float64\n",
      " 6   Sunshine       75625 non-null   float64\n",
      " 7   WindGustDir    135134 non-null  object \n",
      " 8   WindGustSpeed  135197 non-null  float64\n",
      " 9   WindDir9am     134894 non-null  object \n",
      " 10  WindDir3pm     141232 non-null  object \n",
      " 11  WindSpeed9am   143693 non-null  float64\n",
      " 12  WindSpeed3pm   142398 non-null  float64\n",
      " 13  Humidity9am    142806 non-null  float64\n",
      " 14  Humidity3pm    140953 non-null  float64\n",
      " 15  Pressure9am    130395 non-null  float64\n",
      " 16  Pressure3pm    130432 non-null  float64\n",
      " 17  Cloud9am       89572 non-null   float64\n",
      " 18  Cloud3pm       86102 non-null   float64\n",
      " 19  Temp9am        143693 non-null  float64\n",
      " 20  Temp3pm        141851 non-null  float64\n",
      " 21  RainToday      142199 non-null  object \n",
      " 22  RainTomorrow   142193 non-null  object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 25.5+ MB\n",
      "WindGustDir has 16 values\n",
      "Dropping WindGustDir\n",
      "WindDir9am has 16 values\n",
      "Dropping WindDir9am\n",
      "WindDir3pm has 16 values\n",
      "Dropping WindDir3pm\n",
      "Filling missing values in MinTemp\n",
      "Filling missing values in MaxTemp\n",
      "Filling missing values in Rainfall\n",
      "Filling missing values in WindGustSpeed\n",
      "Filling missing values in WindSpeed9am\n",
      "Filling missing values in WindSpeed3pm\n",
      "Filling missing values in Humidity9am\n",
      "Filling missing values in Humidity3pm\n",
      "Filling missing values in Pressure9am\n",
      "Filling missing values in Pressure3pm\n",
      "Filling missing values in Temp9am\n",
      "Filling missing values in Temp3pm\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 140787 entries, 0 to 145458\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count   Dtype         \n",
      "---  ------         --------------   -----         \n",
      " 0   Date           140787 non-null  datetime64[ns]\n",
      " 1   Location       140787 non-null  object        \n",
      " 2   MinTemp        140787 non-null  float64       \n",
      " 3   MaxTemp        140787 non-null  float64       \n",
      " 4   Rainfall       140787 non-null  float64       \n",
      " 5   WindGustSpeed  140787 non-null  float64       \n",
      " 6   WindSpeed9am   140787 non-null  float64       \n",
      " 7   WindSpeed3pm   140787 non-null  float64       \n",
      " 8   Humidity9am    140787 non-null  float64       \n",
      " 9   Humidity3pm    140787 non-null  float64       \n",
      " 10  Pressure9am    140787 non-null  float64       \n",
      " 11  Pressure3pm    140787 non-null  float64       \n",
      " 12  Temp9am        140787 non-null  float64       \n",
      " 13  Temp3pm        140787 non-null  float64       \n",
      " 14  RainToday      140787 non-null  int64         \n",
      " 15  RainTomorrow   140787 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(12), int64(2), object(1)\n",
      "memory usage: 22.3+ MB\n",
      "{'Katherine', 'Hobart', 'Williamtown', 'BadgerysCreek', 'Tuggeranong', 'Cobar', 'NorfolkIsland', 'Wollongong', 'Portland', 'Penrith', 'MountGinini', 'Melbourne', 'GoldCoast', 'Mildura', 'Darwin', 'Uluru', 'SalmonGums', 'Richmond', 'Cairns', 'Walpole', 'Ballarat', 'MountGambier', 'Albury', 'WaggaWagga', 'Watsonia', 'Launceston', 'Sydney', 'NorahHead', 'Woomera', 'CoffsHarbour', 'Perth', 'Canberra', 'Townsville', 'PerthAirport', 'Sale', 'SydneyAirport', 'Nhil', 'Nuriootpa', 'AliceSprings', 'Dartmoor', 'Moree', 'Albany', 'Witchcliffe', 'MelbourneAirport', 'Adelaide', 'PearceRAAF', 'Newcastle', 'Brisbane', 'Bendigo'}\n",
      "{'Katherine': 0, 'Hobart': 1, 'Williamtown': 2, 'BadgerysCreek': 3, 'Tuggeranong': 4, 'Cobar': 5, 'NorfolkIsland': 6, 'Wollongong': 7, 'Portland': 8, 'Penrith': 9, 'MountGinini': 10, 'Melbourne': 11, 'GoldCoast': 12, 'Mildura': 13, 'Darwin': 14, 'Uluru': 15, 'SalmonGums': 16, 'Richmond': 17, 'Cairns': 18, 'Walpole': 19, 'Ballarat': 20, 'MountGambier': 21, 'Albury': 22, 'WaggaWagga': 23, 'Watsonia': 24, 'Launceston': 25, 'Sydney': 26, 'NorahHead': 27, 'Woomera': 28, 'CoffsHarbour': 29, 'Perth': 30, 'Canberra': 31, 'Townsville': 32, 'PerthAirport': 33, 'Sale': 34, 'SydneyAirport': 35, 'Nhil': 36, 'Nuriootpa': 37, 'AliceSprings': 38, 'Dartmoor': 39, 'Moree': 40, 'Albany': 41, 'Witchcliffe': 42, 'MelbourneAirport': 43, 'Adelaide': 44, 'PearceRAAF': 45, 'Newcastle': 46, 'Brisbane': 47, 'Bendigo': 48}\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33613\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - Recall: 0.0937 - accuracy: 0.7353 - loss: 0.5709 - val_Recall: 0.3439 - val_accuracy: 0.8223 - val_loss: 0.4054\n",
      "Epoch 2/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.4006 - accuracy: 0.8286 - loss: 0.3917 - val_Recall: 0.4811 - val_accuracy: 0.8419 - val_loss: 0.3701\n",
      "Epoch 3/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.4872 - accuracy: 0.8438 - loss: 0.3683 - val_Recall: 0.4802 - val_accuracy: 0.8453 - val_loss: 0.3630\n",
      "Epoch 4/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.4904 - accuracy: 0.8433 - loss: 0.3643 - val_Recall: 0.4656 - val_accuracy: 0.8468 - val_loss: 0.3605\n",
      "Epoch 5/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.4908 - accuracy: 0.8455 - loss: 0.3614 - val_Recall: 0.5151 - val_accuracy: 0.8486 - val_loss: 0.3574\n",
      "Epoch 6/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.4964 - accuracy: 0.8482 - loss: 0.3544 - val_Recall: 0.5036 - val_accuracy: 0.8489 - val_loss: 0.3556\n",
      "Epoch 7/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.4936 - accuracy: 0.8471 - loss: 0.3565 - val_Recall: 0.4850 - val_accuracy: 0.8498 - val_loss: 0.3543\n",
      "Epoch 8/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5052 - accuracy: 0.8501 - loss: 0.3540 - val_Recall: 0.5166 - val_accuracy: 0.8502 - val_loss: 0.3531\n",
      "Epoch 9/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5051 - accuracy: 0.8486 - loss: 0.3523 - val_Recall: 0.5000 - val_accuracy: 0.8502 - val_loss: 0.3521\n",
      "Epoch 10/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5106 - accuracy: 0.8489 - loss: 0.3542 - val_Recall: 0.5311 - val_accuracy: 0.8494 - val_loss: 0.3526\n",
      "Epoch 11/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5080 - accuracy: 0.8527 - loss: 0.3495 - val_Recall: 0.5064 - val_accuracy: 0.8501 - val_loss: 0.3508\n",
      "Epoch 12/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5071 - accuracy: 0.8502 - loss: 0.3516 - val_Recall: 0.4866 - val_accuracy: 0.8500 - val_loss: 0.3506\n",
      "Epoch 13/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5202 - accuracy: 0.8519 - loss: 0.3472 - val_Recall: 0.5407 - val_accuracy: 0.8502 - val_loss: 0.3509\n",
      "Epoch 14/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - Recall: 0.5112 - accuracy: 0.8517 - loss: 0.3458 - val_Recall: 0.4876 - val_accuracy: 0.8508 - val_loss: 0.3496\n",
      "Epoch 15/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5106 - accuracy: 0.8523 - loss: 0.3461 - val_Recall: 0.5282 - val_accuracy: 0.8505 - val_loss: 0.3494\n",
      "Epoch 16/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5135 - accuracy: 0.8515 - loss: 0.3464 - val_Recall: 0.5050 - val_accuracy: 0.8512 - val_loss: 0.3487\n",
      "Epoch 17/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5159 - accuracy: 0.8503 - loss: 0.3480 - val_Recall: 0.5300 - val_accuracy: 0.8503 - val_loss: 0.3494\n",
      "Epoch 18/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5132 - accuracy: 0.8521 - loss: 0.3454 - val_Recall: 0.5128 - val_accuracy: 0.8515 - val_loss: 0.3483\n",
      "Epoch 19/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5185 - accuracy: 0.8528 - loss: 0.3456 - val_Recall: 0.5221 - val_accuracy: 0.8512 - val_loss: 0.3482\n",
      "Epoch 20/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5173 - accuracy: 0.8535 - loss: 0.3433 - val_Recall: 0.5338 - val_accuracy: 0.8519 - val_loss: 0.3482\n",
      "Epoch 21/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5108 - accuracy: 0.8512 - loss: 0.3481 - val_Recall: 0.5364 - val_accuracy: 0.8514 - val_loss: 0.3479\n",
      "Epoch 22/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5264 - accuracy: 0.8532 - loss: 0.3443 - val_Recall: 0.5487 - val_accuracy: 0.8512 - val_loss: 0.3483\n",
      "Epoch 23/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5262 - accuracy: 0.8528 - loss: 0.3441 - val_Recall: 0.5073 - val_accuracy: 0.8522 - val_loss: 0.3470\n",
      "Epoch 24/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5200 - accuracy: 0.8533 - loss: 0.3421 - val_Recall: 0.5206 - val_accuracy: 0.8520 - val_loss: 0.3468\n",
      "Epoch 25/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5212 - accuracy: 0.8549 - loss: 0.3412 - val_Recall: 0.5197 - val_accuracy: 0.8520 - val_loss: 0.3466\n",
      "Epoch 26/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5260 - accuracy: 0.8548 - loss: 0.3411 - val_Recall: 0.5394 - val_accuracy: 0.8516 - val_loss: 0.3476\n",
      "Epoch 27/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5254 - accuracy: 0.8537 - loss: 0.3439 - val_Recall: 0.5677 - val_accuracy: 0.8502 - val_loss: 0.3502\n",
      "Epoch 28/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5264 - accuracy: 0.8542 - loss: 0.3411 - val_Recall: 0.5313 - val_accuracy: 0.8524 - val_loss: 0.3464\n",
      "Epoch 29/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5293 - accuracy: 0.8549 - loss: 0.3388 - val_Recall: 0.5438 - val_accuracy: 0.8516 - val_loss: 0.3469\n",
      "Epoch 30/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5252 - accuracy: 0.8533 - loss: 0.3431 - val_Recall: 0.4892 - val_accuracy: 0.8514 - val_loss: 0.3479\n",
      "Epoch 31/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5236 - accuracy: 0.8546 - loss: 0.3397 - val_Recall: 0.5157 - val_accuracy: 0.8517 - val_loss: 0.3461\n",
      "Epoch 32/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5282 - accuracy: 0.8555 - loss: 0.3395 - val_Recall: 0.4997 - val_accuracy: 0.8511 - val_loss: 0.3467\n",
      "Epoch 33/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5210 - accuracy: 0.8544 - loss: 0.3387 - val_Recall: 0.5014 - val_accuracy: 0.8516 - val_loss: 0.3459\n",
      "Epoch 34/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5219 - accuracy: 0.8561 - loss: 0.3375 - val_Recall: 0.4730 - val_accuracy: 0.8512 - val_loss: 0.3494\n",
      "Epoch 35/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5277 - accuracy: 0.8528 - loss: 0.3406 - val_Recall: 0.5004 - val_accuracy: 0.8504 - val_loss: 0.3467\n",
      "Epoch 36/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5292 - accuracy: 0.8582 - loss: 0.3342 - val_Recall: 0.5537 - val_accuracy: 0.8511 - val_loss: 0.3470\n",
      "Epoch 37/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5252 - accuracy: 0.8544 - loss: 0.3394 - val_Recall: 0.5449 - val_accuracy: 0.8508 - val_loss: 0.3461\n",
      "Epoch 38/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5259 - accuracy: 0.8559 - loss: 0.3401 - val_Recall: 0.5657 - val_accuracy: 0.8514 - val_loss: 0.3471\n",
      "Epoch 39/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5344 - accuracy: 0.8546 - loss: 0.3377 - val_Recall: 0.4633 - val_accuracy: 0.8502 - val_loss: 0.3476\n",
      "Epoch 40/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5341 - accuracy: 0.8574 - loss: 0.3362 - val_Recall: 0.5200 - val_accuracy: 0.8515 - val_loss: 0.3450\n",
      "Epoch 41/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5343 - accuracy: 0.8529 - loss: 0.3410 - val_Recall: 0.5613 - val_accuracy: 0.8507 - val_loss: 0.3469\n",
      "Epoch 42/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5345 - accuracy: 0.8564 - loss: 0.3378 - val_Recall: 0.5348 - val_accuracy: 0.8515 - val_loss: 0.3448\n",
      "Epoch 43/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5351 - accuracy: 0.8562 - loss: 0.3368 - val_Recall: 0.5609 - val_accuracy: 0.8513 - val_loss: 0.3465\n",
      "Epoch 44/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5279 - accuracy: 0.8559 - loss: 0.3357 - val_Recall: 0.5290 - val_accuracy: 0.8513 - val_loss: 0.3448\n",
      "Epoch 45/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5367 - accuracy: 0.8571 - loss: 0.3335 - val_Recall: 0.5068 - val_accuracy: 0.8518 - val_loss: 0.3453\n",
      "Epoch 46/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5362 - accuracy: 0.8571 - loss: 0.3342 - val_Recall: 0.4722 - val_accuracy: 0.8511 - val_loss: 0.3473\n",
      "Epoch 47/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5370 - accuracy: 0.8579 - loss: 0.3329 - val_Recall: 0.5726 - val_accuracy: 0.8512 - val_loss: 0.3485\n",
      "Epoch 48/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5361 - accuracy: 0.8572 - loss: 0.3340 - val_Recall: 0.5252 - val_accuracy: 0.8522 - val_loss: 0.3447\n",
      "Epoch 49/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5390 - accuracy: 0.8586 - loss: 0.3342 - val_Recall: 0.5149 - val_accuracy: 0.8518 - val_loss: 0.3450\n",
      "Epoch 50/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5315 - accuracy: 0.8565 - loss: 0.3362 - val_Recall: 0.5045 - val_accuracy: 0.8522 - val_loss: 0.3458\n",
      "Epoch 51/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5414 - accuracy: 0.8596 - loss: 0.3307 - val_Recall: 0.5295 - val_accuracy: 0.8521 - val_loss: 0.3451\n",
      "Epoch 52/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5322 - accuracy: 0.8564 - loss: 0.3356 - val_Recall: 0.4781 - val_accuracy: 0.8513 - val_loss: 0.3473\n",
      "Epoch 53/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5323 - accuracy: 0.8560 - loss: 0.3364 - val_Recall: 0.5716 - val_accuracy: 0.8513 - val_loss: 0.3474\n",
      "Epoch 54/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5360 - accuracy: 0.8572 - loss: 0.3347 - val_Recall: 0.5236 - val_accuracy: 0.8516 - val_loss: 0.3449\n",
      "Epoch 55/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5424 - accuracy: 0.8586 - loss: 0.3314 - val_Recall: 0.5456 - val_accuracy: 0.8521 - val_loss: 0.3456\n",
      "Epoch 56/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5411 - accuracy: 0.8582 - loss: 0.3342 - val_Recall: 0.4598 - val_accuracy: 0.8508 - val_loss: 0.3485\n",
      "Epoch 57/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5322 - accuracy: 0.8570 - loss: 0.3335 - val_Recall: 0.5357 - val_accuracy: 0.8524 - val_loss: 0.3451\n",
      "Epoch 58/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5410 - accuracy: 0.8590 - loss: 0.3305 - val_Recall: 0.5242 - val_accuracy: 0.8524 - val_loss: 0.3451\n",
      "Epoch 59/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5469 - accuracy: 0.8587 - loss: 0.3304 - val_Recall: 0.5231 - val_accuracy: 0.8517 - val_loss: 0.3438\n",
      "Epoch 60/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5440 - accuracy: 0.8597 - loss: 0.3293 - val_Recall: 0.5410 - val_accuracy: 0.8526 - val_loss: 0.3445\n",
      "Epoch 61/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5447 - accuracy: 0.8589 - loss: 0.3336 - val_Recall: 0.5309 - val_accuracy: 0.8523 - val_loss: 0.3445\n",
      "Epoch 62/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5327 - accuracy: 0.8580 - loss: 0.3319 - val_Recall: 0.5643 - val_accuracy: 0.8518 - val_loss: 0.3462\n",
      "Epoch 63/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5440 - accuracy: 0.8582 - loss: 0.3325 - val_Recall: 0.5598 - val_accuracy: 0.8515 - val_loss: 0.3461\n",
      "Epoch 64/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5424 - accuracy: 0.8593 - loss: 0.3301 - val_Recall: 0.5032 - val_accuracy: 0.8507 - val_loss: 0.3452\n",
      "Epoch 65/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5443 - accuracy: 0.8610 - loss: 0.3264 - val_Recall: 0.5742 - val_accuracy: 0.8512 - val_loss: 0.3477\n",
      "Epoch 66/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5531 - accuracy: 0.8607 - loss: 0.3284 - val_Recall: 0.5275 - val_accuracy: 0.8519 - val_loss: 0.3448\n",
      "Epoch 67/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5436 - accuracy: 0.8598 - loss: 0.3295 - val_Recall: 0.5268 - val_accuracy: 0.8516 - val_loss: 0.3443\n",
      "Epoch 68/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5472 - accuracy: 0.8600 - loss: 0.3295 - val_Recall: 0.5241 - val_accuracy: 0.8519 - val_loss: 0.3454\n",
      "Epoch 69/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5458 - accuracy: 0.8586 - loss: 0.3290 - val_Recall: 0.5245 - val_accuracy: 0.8519 - val_loss: 0.3445\n",
      "Epoch 70/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5396 - accuracy: 0.8577 - loss: 0.3332 - val_Recall: 0.5171 - val_accuracy: 0.8523 - val_loss: 0.3444\n",
      "Epoch 71/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5414 - accuracy: 0.8585 - loss: 0.3321 - val_Recall: 0.5280 - val_accuracy: 0.8521 - val_loss: 0.3445\n",
      "Epoch 72/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5453 - accuracy: 0.8613 - loss: 0.3261 - val_Recall: 0.5295 - val_accuracy: 0.8522 - val_loss: 0.3440\n",
      "Epoch 73/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5458 - accuracy: 0.8577 - loss: 0.3332 - val_Recall: 0.4983 - val_accuracy: 0.8523 - val_loss: 0.3445\n",
      "Epoch 74/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5412 - accuracy: 0.8607 - loss: 0.3267 - val_Recall: 0.5012 - val_accuracy: 0.8522 - val_loss: 0.3451\n",
      "Epoch 75/75\n",
      "\u001b[1m770/770\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - Recall: 0.5521 - accuracy: 0.8592 - loss: 0.3287 - val_Recall: 0.4981 - val_accuracy: 0.8527 - val_loss: 0.3442\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1320/1320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91     32915\n",
      "           1       0.75      0.50      0.60      9322\n",
      "\n",
      "    accuracy                           0.85     42237\n",
      "   macro avg       0.81      0.73      0.75     42237\n",
      "weighted avg       0.84      0.85      0.84     42237\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90     32915\n",
      "           1       0.72      0.47      0.57      9322\n",
      "\n",
      "    accuracy                           0.84     42237\n",
      "   macro avg       0.79      0.71      0.74     42237\n",
      "weighted avg       0.83      0.84      0.83     42237\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31171</td>\n",
       "      <td>1744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4914</td>\n",
       "      <td>4408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1\n",
       "0  31171  1744\n",
       "1   4914  4408"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Chargez le jeu de données et préparez-le.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"../data/weatherAUS.csv\")\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.Location.unique()\n",
    "\n",
    "df.isnull().sum() / df.shape[0] *100\n",
    "\n",
    "# On drop les colonnes avec trop de valeurs manquantes\n",
    "df.drop(['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'], axis=1, inplace=True)\n",
    "\n",
    "# Valeurs nulles dans la cible à prédire (RainTomorrow)\n",
    "df.dropna(subset=[\"RainTomorrow\", \"RainToday\"], inplace=True)\n",
    "\n",
    "# Pour chaque colonne \n",
    "for c in df.columns :\n",
    "    # Si la colonne est categorielle ?\n",
    "    if df[c].dtype.name == 'object':\n",
    "        if df[c].isnull().sum() > 0:\n",
    "            n = df[c].nunique()\n",
    "            print(c, \"has\", n, \"values\")\n",
    "            if n > 5:\n",
    "                print(\"Dropping\", c)\n",
    "                df.drop(c, axis=1, inplace=True)\n",
    "            else:\n",
    "                print(\"Filling missing values\")\n",
    "                df[c].fillna(df[c].mode(), inplace=True)\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"Month\"] = df.Date.dt.month\n",
    "\n",
    "cities = df[(df.groupby([\"Month\", \"Location\"])[\"Pressure9am\"].transform(\"mean\")).isnull()].Location.unique()\n",
    "df[df.Location.isin(cities)]\n",
    "\n",
    "# Remplissage des valeurs nulles dans les colonnes numeriques\n",
    "for nm in df.columns:\n",
    "    # Si la colonne est bien numerique\n",
    "    if type(df[nm][0])==np.float64:\n",
    "        print(\"Filling missing values in\",nm)\n",
    "        df[\"tmp\"]=df.groupby([\"Month\", \"Location\"])[nm].transform(\"mean\")\n",
    "        df.loc[df[nm].isna(),nm] = df.loc[df[nm].isna(),\"tmp\"]\n",
    "        # Si aucune valeur par ville on prend la moyenne du pays :\n",
    "        if df[nm].isnull().sum()>0:\n",
    "            df[\"tmp\"]=df.groupby([\"Month\"])[nm].transform(\"mean\")\n",
    "            df.loc[df[nm].isna(),nm] = df.loc[df[nm].isna(),\"tmp\"]\n",
    "        df=df.drop(\"tmp\",axis=1)\n",
    "df.drop(\"Month\", axis=1, inplace=True)\n",
    "df.isna().sum()\n",
    "\n",
    "# On recode \"RainToday\" et \"RainTomorrow\" en numerique\n",
    "for c in [\"RainToday\", \"RainTomorrow\"]:\n",
    "    df[c].replace(\"Yes\", 1, inplace=True)\n",
    "    df[c].replace(\"No\", 0, inplace=True)\n",
    "df.info()\n",
    "\n",
    "# Gestion de la derniere variable categorielle : \"Location\"\n",
    "myset = set( df.Location)\n",
    "print(myset)\n",
    "dic_location={}\n",
    "j=0\n",
    "for i in myset:\n",
    "    dic_location[i]=j\n",
    "    j+=1\n",
    "print(dic_location)\n",
    "df['Location']=df.Location.map(dic_location)\n",
    "\n",
    "# Split des donnees\n",
    "X, y = df.drop([\"RainTomorrow\", \"Date\"], axis=1), df[\"RainTomorrow\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2. Modélisez un réseau de neurones à cinq couches avec 100 neurones chacun et entraînez-le.\n",
    "import tensorflow as tf\n",
    "# Definition de la fonction de création\n",
    "def model(input_dim):\n",
    "    # Nous créons un modèle dit séquentiel\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Ajouter le premier calque \"Dense\" de 100 unités, \n",
    "#     et donner la dimension d'entrée (ici 5)\n",
    "    model.add(tf.keras.layers.Dense(100, input_dim=input_dim, \n",
    "                                    activation='relu'))\n",
    "    # Ajouter le deuxième calque \"Dense\" de 100 unités\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "    # Ajouter enfin la couche de sortie avec une unité: le résultat prédit\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # renvoie le modèle créé\n",
    "    return model  \n",
    "\n",
    "# Creation du modèle\n",
    "my_model = model(input_dim=X.shape[1])\n",
    "# Compilation :\n",
    "my_model.compile(optimizer=\"SGD\", loss=\"binary_crossentropy\", \n",
    "                 metrics=[\"accuracy\", \"Recall\"])\n",
    "\n",
    "my_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "            epochs=75, batch_size=128)\n",
    "\n",
    "y_pred = my_model.predict(X_test)\n",
    "\n",
    "# Test du modele de deep learning\n",
    "y_pred = my_model.predict(X_test)>0.5\n",
    "print(classification_report(y_test, y_pred))\n",
    "pd.DataFrame(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# 3. Comparez vos résultats avec une régression logistique classique.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "pd.DataFrame(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 3 : MLP à la mode de chez nous\n",
    "\n",
    "<img src=\"https://www.linfodurable.fr/sites/linfodurable/files/styles/landscape_w800/public/2019-05/v%C3%AAtements.jpg?h=983a09bf&itok=28u6sVQQ\" width=60%>\n",
    "\n",
    "Dans cet exercice, nous allons essayer d'utiliser un réseau de neurones sur une tâche de classification un peu moins simple : classer des images de vêtements en 10 classes.\n",
    "\n",
    "Nous allons d'abord télécharger les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "# Recuperation des données sous forme de tableaux numpy\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet ensemble de données contient 10 classes :\n",
    "    \n",
    "    \n",
    "- **0** : T-shirt/top - T-shirt / haut\n",
    "- **1** : Trouser - pantalon\n",
    "- **2** : Pullover - Pull\n",
    "- **3** : Dress - robe\n",
    "- **4** : Coat - manteau\n",
    "- **5** : Sandal - sandale\n",
    "- **6** : Shirt - chemise\n",
    "- **7** : Sneaker - baskets\n",
    "- **8** : Bag - sac\n",
    "- **9** : Ankle boot - bottine\n",
    "\n",
    "Commencez maintenant par explorer les données. Essayez d'afficher certaines images avec le label associé. \n",
    "\n",
    "> Astuce : regarder la fonction **`imshow`** de matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explorer les données. Essayez d'afficher certaines images avec le label associé\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>123</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>156</td>\n",
       "      <td>161</td>\n",
       "      <td>109</td>\n",
       "      <td>64</td>\n",
       "      <td>23</td>\n",
       "      <td>77</td>\n",
       "      <td>130</td>\n",
       "      <td>72</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>216</td>\n",
       "      <td>163</td>\n",
       "      <td>127</td>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>146</td>\n",
       "      <td>141</td>\n",
       "      <td>88</td>\n",
       "      <td>172</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>223</td>\n",
       "      <td>223</td>\n",
       "      <td>215</td>\n",
       "      <td>213</td>\n",
       "      <td>164</td>\n",
       "      <td>127</td>\n",
       "      <td>123</td>\n",
       "      <td>196</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>227</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "      <td>224</td>\n",
       "      <td>221</td>\n",
       "      <td>223</td>\n",
       "      <td>245</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>212</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>213</td>\n",
       "      <td>223</td>\n",
       "      <td>220</td>\n",
       "      <td>243</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>169</td>\n",
       "      <td>227</td>\n",
       "      <td>208</td>\n",
       "      <td>218</td>\n",
       "      <td>224</td>\n",
       "      <td>212</td>\n",
       "      <td>226</td>\n",
       "      <td>197</td>\n",
       "      <td>209</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>198</td>\n",
       "      <td>221</td>\n",
       "      <td>215</td>\n",
       "      <td>213</td>\n",
       "      <td>222</td>\n",
       "      <td>220</td>\n",
       "      <td>245</td>\n",
       "      <td>119</td>\n",
       "      <td>167</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>232</td>\n",
       "      <td>213</td>\n",
       "      <td>218</td>\n",
       "      <td>223</td>\n",
       "      <td>234</td>\n",
       "      <td>217</td>\n",
       "      <td>217</td>\n",
       "      <td>209</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>222</td>\n",
       "      <td>221</td>\n",
       "      <td>216</td>\n",
       "      <td>223</td>\n",
       "      <td>229</td>\n",
       "      <td>215</td>\n",
       "      <td>218</td>\n",
       "      <td>255</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>211</td>\n",
       "      <td>218</td>\n",
       "      <td>224</td>\n",
       "      <td>223</td>\n",
       "      <td>219</td>\n",
       "      <td>215</td>\n",
       "      <td>224</td>\n",
       "      <td>244</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>44</td>\n",
       "      <td>82</td>\n",
       "      <td>107</td>\n",
       "      <td>189</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>234</td>\n",
       "      <td>176</td>\n",
       "      <td>188</td>\n",
       "      <td>250</td>\n",
       "      <td>248</td>\n",
       "      <td>233</td>\n",
       "      <td>238</td>\n",
       "      <td>215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>187</td>\n",
       "      <td>208</td>\n",
       "      <td>224</td>\n",
       "      <td>221</td>\n",
       "      <td>224</td>\n",
       "      <td>208</td>\n",
       "      <td>204</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>221</td>\n",
       "      <td>234</td>\n",
       "      <td>221</td>\n",
       "      <td>211</td>\n",
       "      <td>220</td>\n",
       "      <td>232</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>202</td>\n",
       "      <td>228</td>\n",
       "      <td>224</td>\n",
       "      <td>221</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>214</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>...</td>\n",
       "      <td>188</td>\n",
       "      <td>154</td>\n",
       "      <td>191</td>\n",
       "      <td>210</td>\n",
       "      <td>204</td>\n",
       "      <td>209</td>\n",
       "      <td>222</td>\n",
       "      <td>228</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>98</td>\n",
       "      <td>233</td>\n",
       "      <td>198</td>\n",
       "      <td>210</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>229</td>\n",
       "      <td>234</td>\n",
       "      <td>249</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>219</td>\n",
       "      <td>221</td>\n",
       "      <td>215</td>\n",
       "      <td>217</td>\n",
       "      <td>223</td>\n",
       "      <td>223</td>\n",
       "      <td>224</td>\n",
       "      <td>229</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>75</td>\n",
       "      <td>204</td>\n",
       "      <td>212</td>\n",
       "      <td>204</td>\n",
       "      <td>193</td>\n",
       "      <td>205</td>\n",
       "      <td>211</td>\n",
       "      <td>225</td>\n",
       "      <td>216</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>239</td>\n",
       "      <td>223</td>\n",
       "      <td>218</td>\n",
       "      <td>212</td>\n",
       "      <td>209</td>\n",
       "      <td>222</td>\n",
       "      <td>220</td>\n",
       "      <td>221</td>\n",
       "      <td>230</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>48</td>\n",
       "      <td>203</td>\n",
       "      <td>183</td>\n",
       "      <td>194</td>\n",
       "      <td>213</td>\n",
       "      <td>197</td>\n",
       "      <td>185</td>\n",
       "      <td>190</td>\n",
       "      <td>194</td>\n",
       "      <td>192</td>\n",
       "      <td>...</td>\n",
       "      <td>199</td>\n",
       "      <td>206</td>\n",
       "      <td>186</td>\n",
       "      <td>181</td>\n",
       "      <td>177</td>\n",
       "      <td>172</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>206</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>219</td>\n",
       "      <td>193</td>\n",
       "      <td>179</td>\n",
       "      <td>171</td>\n",
       "      <td>183</td>\n",
       "      <td>196</td>\n",
       "      <td>204</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>195</td>\n",
       "      <td>191</td>\n",
       "      <td>198</td>\n",
       "      <td>192</td>\n",
       "      <td>176</td>\n",
       "      <td>156</td>\n",
       "      <td>167</td>\n",
       "      <td>177</td>\n",
       "      <td>210</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>189</td>\n",
       "      <td>212</td>\n",
       "      <td>191</td>\n",
       "      <td>175</td>\n",
       "      <td>172</td>\n",
       "      <td>175</td>\n",
       "      <td>181</td>\n",
       "      <td>...</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>194</td>\n",
       "      <td>192</td>\n",
       "      <td>216</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>200</td>\n",
       "      <td>222</td>\n",
       "      <td>237</td>\n",
       "      <td>239</td>\n",
       "      <td>242</td>\n",
       "      <td>...</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>181</td>\n",
       "      <td>176</td>\n",
       "      <td>166</td>\n",
       "      <td>168</td>\n",
       "      <td>99</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>61</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...   18   19   20   21  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...    0    1    4    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...   54    0    0    0   \n",
       "5    0    0    0    0    0    0    0    0    0    0  ...  144  123   23    0   \n",
       "6    0    0    0    0    0    0    0    0    0    0  ...  107  156  161  109   \n",
       "7    0    0    0    0    0    0    0    0    0    0  ...  216  163  127  121   \n",
       "8    0    0    0    0    0    0    0    0    0    1  ...  223  223  215  213   \n",
       "9    0    0    0    0    0    0    0    0    0    0  ...  235  227  224  222   \n",
       "10   0    0    0    0    0    0    0    0    0    0  ...  180  212  210  211   \n",
       "11   0    0    0    0    0    0    0    0    0    1  ...  169  227  208  218   \n",
       "12   0    0    0    0    0    0    0    0    0    0  ...  198  221  215  213   \n",
       "13   0    0    0    0    0    0    0    0    0    4  ...  232  213  218  223   \n",
       "14   0    0    1    4    6    7    2    0    0    0  ...  222  221  216  223   \n",
       "15   0    3    0    0    0    0    0    0    0   62  ...  211  218  224  223   \n",
       "16   0    0    0    0   18   44   82  107  189  228  ...  224  234  176  188   \n",
       "17   0   57  187  208  224  221  224  208  204  214  ...  255  255  221  234   \n",
       "18   3  202  228  224  221  211  211  214  205  205  ...  188  154  191  210   \n",
       "19  98  233  198  210  222  229  229  234  249  220  ...  168  219  221  215   \n",
       "20  75  204  212  204  193  205  211  225  216  185  ...  239  223  218  212   \n",
       "21  48  203  183  194  213  197  185  190  194  192  ...  199  206  186  181   \n",
       "22   0  122  219  193  179  171  183  196  204  210  ...  195  191  198  192   \n",
       "23   0    0   74  189  212  191  175  172  175  181  ...  210  210  211  188   \n",
       "24   2    0    0    0   66  200  222  237  239  242  ...  182  182  181  176   \n",
       "25   0    0    0    0    0    0    0   40   61   44  ...    0    0    0    0   \n",
       "26   0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "27   0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "     22   23   24   25   26   27  \n",
       "0     0    0    0    0    0    0  \n",
       "1     0    0    0    0    0    0  \n",
       "2     0    0    0    0    0    0  \n",
       "3     0    0    0    1    1    0  \n",
       "4     1    3    4    0    0    3  \n",
       "5     0    0    0   12   10    0  \n",
       "6    64   23   77  130   72   15  \n",
       "7   122  146  141   88  172   66  \n",
       "8   164  127  123  196  229    0  \n",
       "9   224  221  223  245  173    0  \n",
       "10  213  223  220  243  202    0  \n",
       "11  224  212  226  197  209   52  \n",
       "12  222  220  245  119  167   56  \n",
       "13  234  217  217  209   92    0  \n",
       "14  229  215  218  255   77    0  \n",
       "15  219  215  224  244  159    0  \n",
       "16  250  248  233  238  215    0  \n",
       "17  221  211  220  232  246    0  \n",
       "18  204  209  222  228  225    0  \n",
       "19  217  223  223  224  229   29  \n",
       "20  209  222  220  221  230   67  \n",
       "21  177  172  181  205  206  115  \n",
       "22  176  156  167  177  210   92  \n",
       "23  188  194  192  216  170    0  \n",
       "24  166  168   99   58    0    0  \n",
       "25    0    0    0    0    0    0  \n",
       "26    0    0    0    0    0    0  \n",
       "27    0    0    0    0    0    0  \n",
       "\n",
       "[28 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Que donne une image en pixels ? Un tableau en 2D car nous sommes en niveau de gris\n",
    "# 0 : pixel blanc, 255: pixel noir\n",
    "pd.DataFrame(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4sAAAHNCAYAAAC3je47AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABNZElEQVR4nO3deXhdZbn///spHZM0SdN0SpNOUGjLVKiMF9gCwhFw4Bw4iuJB5Kj49QceEBX9iooiHEQEEXDiKBxBQHG6ZEaxQhkFFahIaemYpmkzNW3SeXh+f+zdr7FP7k/LatLspO/XdfVS9idr7bXXXs9a68neue8QYzQAAAAAADrq19MbAAAAAAAoPEwWAQAAAAAJJosAAAAAgASTRQAAAABAgskiAAAAACDBZBEAAAAAkGCyuJeFEJaEEN7hZCeGEN7Y29sEoOupsQ6g5zFGAWDXmCzuphBCe4d/20MIGzr893ld8RwxxjkxxoN2sR2dXtxCCB8MIdwTQpgQQoghhP5dsU1AXxBCOCGE8GwIYU0IoSWE8EwI4aie3i4AOYxRoG/J36/uuFdeHUJ4KIRQ09PbhbeOyeJuijGW7PhnZsvM7N0dHvtpdz//bkz+zjCzh7t7O4DeJoRQamYPmtktZlZhZmPN7Ktmtqknt2t38Esf7AsYo0Cf9e78ffMYM1tluTGOXobJYjcIIVSGEB4MIbTmf0M6J4TQcV9PDyG8mv8N6s9CCIPzy80KISzvsJ4lIYQrQgivmtm6EMK9ZjbOzB7I/6bmc/mf62dmp5rZo2b2VH7x1vzPHBdC6BdCuDKEsDSE0BBC+EkIoSy/7I5PIj8eQlgRQqgPIVze/XsJ2GsONDOLMd4bY9wWY9wQY3w8xvhqCOGCEMLTIYQb8r/5XBxCOH3HgiGEshDCj/Ljoi6E8PUQwn75bP8Qwh9CCM0hhKYQwk9DCOWdbUAIYUp+3efm//tdIYSX8+eIZ0MIh3X42Z3HPTej6OsYo0AfFmPcaGa/MLNpZmYhhDNDCH8NIawNIdSGEK7q+PMhhPPz96zNIYQvBb4y3qOYLHaPy81suZmNMLNRZvZ/zSx2yN9nZu80s4lmdpiZXSDW9QEzO9PMymOMH7B//lTz+vzPHG1mi2KMTWb29vxj5fmfeS6//gvM7CQzm2RmJWZ2607Pc5KZTTaz08zs8wxK9CHzzWxbCOF/QwinhxCG7ZQfY2ZvmFmlmV1vZj8KIYR89r9mttXMDjCzIyw3Pj6az4KZ/beZVZnZVDOrMbOrdn7yEMKRZva4mV0SY7wv/98/NrOLzGy4mf3AzH4bQhjUYbGO437rHrx2oDdgjAJ9WAihyMzeb2bP5x9aZ2bnm1m55cbR/wkhnJX/2Wlm9l0zO89yn0iWWe7bBughTBa7xxbLHeDjY4xb8n+L2HGy+J0Y44oYY4uZPWBm08W6vhNjrI0xbhA/c6bpr6CeZ2Y3xhgXxRjbzewLZnbuTr8N/WqMcV2Mca6Z3WG5CyHQ68UY15rZCZb7hc3tZtYYQvhtCGFU/keWxhhvjzFus9yN5xgzG5XPTzezS/Njo8HMbjKzc/PrfTPG+LsY46YYY6OZ3WhmM3d6+hPN7Ldm9uEY44P5xz5mZj+IMb6Q/xTlfy33dbtjOyy3O+Me6BMYo0Cf9ZsQQquZrbXcN+C+aWYWY/xjjHFujHF7jPFVM7vX/jE2zzGzB2KMT8cYN5vZl+2fP3DBXsZkcQ+FEMaFDsVv8g9/08zeNLPHQwiLQgif32mxlR3+/3rLfdLnqd2NzdjV3ytWmdnSDv+91Mz6W+5Tz86eZ2l+GaBPiDG+HmO8IMZYbWaHWO74/nY+Xtnh59bn/2+JmY03swFmVp//Klqr5T5hGGlmFkIYGUK4L//Vt7VmdrflPvno6BNm9myMcXaHx8ab2eU71plfb43985jbnXEP9BmMUaBPOivGWG5mg8zsYjN7MoQwOoRwTAhhdgihMYSwxnLjcMfYrLIO4ys/5pv38najAyaLeyjGuGyn4jcWY2yLMV4eY5xkZu82s0+HEE7J+hTqv0MIoy33W9a/OD9vZrbCche/HcZZ7ms7qzo8VrNTviLLxgKFLsY4z8zutNwNqVJruU8TKmOM5fl/pTHGg/P5f1tuvB0WYyw1sw9Z7mtvHX3CzMaFEG7aab3XdFhneYyxKMZ4b8fNzPbqgN6PMQr0LflP6H9lZtss9y2Ceyz3iX5NjLHMzL5v/xib9WZWvWPZEMIQy30dHD2EyWI3yP9h/AH5v6lYa7nBsa2LVr/Kcn93uMMZZvZoh6+5NprZ9p1+5l4zuyyEMDGEUGJm15rZz3b6O4svhRCKQggHm9lHzOxnXbS9QI/KF664PIRQnf/vGst9zfp5tVyMsd5yf8f0rRBCacgVito/hLDjqzJDzazdcsWkxprZZztZTZvl/j757SGE6/KP3W5mn8j/ZjWEEIrzf+w/dI9fLNALMUaBvi0/jt5rZsPM7HXLjc2WGOPGEMLRZvbBDj/+CzN7dwjh+BDCQMtVRt75lzzYi5gsdo/JZvZ7y12knjOz78YY/9hF6/5vM7sy/9WYz9hOX0HNf1x/jZk9k/+ZYy33h/p3Wa5S6mIz22hml+y03ict99XZJ8zshhjj4120vUBPa7NcgYwXQgjrLHcD+jfLFaLalfPNbKCZ/d3MVlvuIjYmn33VzI40szVm9pCZ/aqzFcQYWy33txqnhxCujjG+ZLm/ibo1v843TRe5Avo6xijQNz2Q/xOttZa7N/1wjPE1M/ukmX0thNBmub9J/PmOBfL5JWZ2n+U+ZWwzswbrBa10+qrwz3VX0JvkC9SsNLP9Y4xrMq5jguUmkAOo6AYAAIBCkf9GXKuZTY4xLu7hzdkn8cli71ZhZl/KOlEEAAAACkkI4d35P40qNrMbzGyumS3p2a3adzFZ7MVijA0xxu/19HYAAAAAXeS9liu0uMJyf9p1buSrkD2Gr6ECAAAAABJ8sggAAAAASDBZBAAAAAAk+u8i5zuqQE6h9vhhjAI5jFGgsDFGgcLW6Rjlk0UAAAAAQILJIgAAAAAgwWQRAAAAAJBgsggAAAAASDBZBAAAAAAkdlUNtU9744033Ozhhx92s+XLl7tZU1OTm1122WVuVlNT0+njgwcPdpdR2//000+72YsvvuhmgwYNcrOqqio3O/nkk93s2GOPdTP1+gAAAAD0HD5ZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIAEk0UAAAAAQILJIgAAAAAgEWKMKpdhV1PbEkJws8bGRje79tpr3ay+vt7Nmpub3eyEE05ws4suusjNHnzwQTf75S9/2enj6rWdeOKJbnbEEUe42UknneRmt956q5vV1ta6WXFxsZu1t7e72SWXXOJmxx9/vJv1AP8A7Fl7dYwCBYwxChQ2xihQ2Dodo3yyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgASTRQAAAABAoqBaZ2T1+c9/3s3WrVvnZuPHj3ezpUuXutnatWvdbObMmW524YUXupln1apVbjZ8+HA327Ztm5s99NBDbvbEE0+42dChQ91s+/btmbZlzZo1bvad73zHzYqKitysm1DyGyhsjFGgsDFGgcJG6wwAAAAAwO5hsggAAAAASDBZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIBEr2mdUVtb62ZXXXWVm40ZM8bNVEuHzZs3u9mAAQPc7JVXXnGzRx991M1ef/31Th+fMmWKu4xqV3Hccce5Wf/+/d3s+OOPd7PBgwe72fz5891MtShpbGx0s+rqaje7+uqr3aybUPIbKGyMUaCwMUaBwkbrDAAAAADA7mGyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgITfQ6HAzJkzx81US4dNmza5mWqrUVdX52aTJk1ys5EjR7rZaaed5maf/OQnO3381ltvdZeZN2+em82YMcPNRo8e7WYTJ050M9UeQ+2T9evXu1lbW5ubPf/8824G7G1em6EQ/GrwqjWRWq6lpcXN7rjjDje7/PLL3ayQ7KJlk0vtMwDoalnP4cq6devcbL/99nOzrVu3Znq+LVu2uJl6faq9nMo2btzoZhs2bOj0cXUfr56rqKioy5dT76tqPaeo9yDra1DbovZnFnyyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgASTRQAAAABAote0zvjd737nZqrsrCrh29rammlbXnzxRTdTbSlUdsIJJ3T6+H/+53+6y1RXV7uZamWxatUqN1NtLoYPH+5mZWVlbnbvvfe62dve9jY3e+yxx9xMtUQZNGiQmwFZZSmTnrW0+n333edmn/vc59xs7Nixbnbuuedm2pbuaHNBCwwAvcH27dvdTLW5aGhocLNPf/rTbjZq1Cg3U/dn/fr5n/2otg2qNYNaLuv5feDAgZ0+vnbtWncZdb+n7kvVPlH3z2o5dT2cPHmym6njQc1h1POpe93zzz/fzbLgk0UAAAAAQILJIgAAAAAgwWQRAAAAAJBgsggAAAAASDBZBAAAAAAkmCwCAAAAABIF1Tqjvb3dzUaMGOFmqrxvW1ubm6m2GqokbWlpqZsNHjzYzTZs2OBmmzdv7vTxj33sY+4yjY2NblZfX+9mAwYMcLOFCxe6WWVlpZvNnj3bzUaOHOlmEyZMcDOvnYiZWVNTk5up9gFAVl4J66xtIFpaWtzsxhtvdDN1fF955ZVudtxxx7nZ+PHj3aw72lyocvSqbDkKjyrt3pdbpKjr7xVXXOFmF110kZsdc8wxe7RNnemO1jfYNdXmon9//9ZbnRvVvduaNWvcTLWeGDp0qJuVlJS4WXNzs5up++6tW7d2+rhqQ6Iy9VyqHUfWNiTevbqZ2aJFi9xMzWHUvEHd66r3p6txVQYAAAAAJJgsAgAAAAASTBYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIFFQrTOWLl3qZrW1tW42btw4N1NtNVTp3BkzZriZV/rXTJc2Vi03vPK4WUvMn3LKKW72xhtvuJnal+Xl5W6WtWyzel9Vq5GHHnrIzT7+8Y+7GVAoTj75ZDfbtm2bm6ky26qcuTqn/fnPf3az7373u272gQ98wM2mT5/uZl/84hfd7MMf/rCbTZkyxc3QMwqpxUJXt7dR7bV+/OMfu5lqvXXhhRe6mRqHatwr3fH+fPvb33azSy+9tMufrydlbeWj7vcWLFiQabnVq1e7mWqRoq4n6v5MtTarq6tzM3XvNmTIkE4fV/e6an3Tpk1zM7VPXnzxRTfbf//93Uzd46tj5a9//aub1dTUuNmKFSvcTN2vdzU+WQQAAAAAJJgsAgAAAAASTBYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIFFQrTMefPBBN1Nlo1WZ6ueee87N3vGOd7jZqlWr3EyV+FXl6V977TU3Ky4u7vTx119/3V1m5MiRbqbKGqsyve3t7W6m2lxUVVW52bx589xMtePw2omYmR1wwAFuBnSHLCXoVRsIdXyPGTPGzZYvX+5mLS0tbnb33Xe7WWNjo5tdf/31bvbYY4+52ejRo91Mtdr5j//4DzdD76JK12dt6dDV67zxxhvd7Atf+IKbqePUaw9gZlZSUuJmJ5xwgpudfvrpbqZaHJSVlbnZscce62ZqP1922WVuptrpjBo1ys0KVdbjVLUo69/fv/VWrSzUe5K19YSirlGqrYw6/r3Xp9Y3adIkN2toaHCzoUOHuplqO6FalBQVFblZa2urm40fP97NFi5c6GbqXkC99vPOO8/NfvrTn7qZh08WAQAAAAAJJosAAAAAgASTRQAAAABAgskiAAAAACDBZBEAAAAAkGCyCAAAAABIBFWK18xkuDepssD9+nX9nPfkk092s7PPPtvNBgwY4GZr1651M69Vhyr9q8oTq3LZqryvKtN71113udkPf/hDN1NlvXuRbPWzu1/BjNF91bPPPutmp556qpupUtrLli1zM9UqaOrUqW72zDPPuJlqTTRr1iw3GzFihJtt27bNzVRLoJtuusnNZs6c6WbGGIVDtYa54oor3EyV0FdtJ15++WU3a25udjNVCn/Lli1upu4F1BhVbRrGjh3rZhUVFW528803u5kV6BiN4kZYtc5Q7THOOeccN5s+fbqbrVixws22bt3qZqqlQ319vZtt3rzZzdT9oLoOZWnVobZDrU/dI2fdX5WVlW6m7p9VSxR1T67mDW1tbZmeTx2bDz30kJtVVVV1esDzySIAAAAAIMFkEQAAAACQYLIIAAAAAEgwWQQAAAAAJJgsAgAAAAASTBYBAAAAAAm/7mqB6Y72GIoqCzxv3jw3U+VxDzjgADdramrq9HFVFnj9+vVutnLlSjfz2nSY6ZLYqjx3aWmpmwHKLtr3uFRJ8+7gld9/z3ve4y4zefJkN1uzZo2bqTL5F198sZvde++9bqbG9kknneRmqrx+XV2dm2Utrb5kyRI320XrDOwDVOn65557rtPHH3nkEXcZdR097rjj3Oz3v/+9m6nxq9rbHHPMMW6mtnPhwoVuplp2FRcXu5lqN6Ja+/RGWa8l1157rZuNHDnSzdQ5VZ03FTUu1DrV8aGO46zLedT1cL/99nMztS9LSkrcrL293c3UtXLgwIGZtkWNUXU9POigg9xMva9qn82ZM8fN3v/+93f6OJ8sAgAAAAASTBYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQKKjWGVlL6CtZSyKrErj19fVu1r+/v0tV6dwBAwZ0+nhDQ4O7zODBg91s0aJFbqbacaiy9RMnTnSzLKWSzfR7vrdbI6BnZH2ft23b5maqbLRy3XXXudnVV1/d6eMTJkxwl5k7d66bqRLcl112mZup80jWNgBquXXr1rmZGr/qPVDnC7XPsGt7+5y6t5/vj3/8o5s98cQTnT6ujuEjjzzSzV588UU3e/rpp91MXWNVu5lhw4a5WVVVlZup1lVjxoxxs40bN7qZupdRbTX6ms2bN7uZ2rctLS1uptpOqHOxui/12q+Z6TGqnq+2ttbNVJs4tc+8sahem7rXzUqNp6zvnWrVoVqpqPGrxppqnaGOB9W+x8MniwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEgwWQQAAAAAJAqqdUYhtUp429ve5marVq1yM/UaVDltb52TJk1yl9mwYYObqZLOavtVOX9Vfnn16tVuNmrUKDcDssraHuOGG25ws1tuucXNqqurO3182bJl7jKVlZVu9slPftLNvBYAZmavvfaam82YMcPNVq5c6WZe655dUcv16+f/LlK9d6+++mqmbUFO1uuoOr+rTL3PWT388MNudv/997vZ/vvv3+njqsz/Cy+84GZLly51s3POOcfN1PVQtbVSZfkV9fpUOw5V6l+1JlLtFvoa1eZnyJAhbqbaGowYMcLN1DGn2pmoc/GKFSvcTB0D7e3tbqbaWahWNd65X22HkvW8tX379kzL1dTUuJm6f25ra3MzdQ9RUVHhZsXFxW6m2n888sgjbnbYYYd1+jifLAIAAAAAEkwWAQAAAAAJJosAAAAAgASTRQAAAABAgskiAAAAACBRUNVQC4mq8KaqXKkqpKpKnVd1SlXiUhXJDj/8cDd75pln3ExVXlLVlVRlVkBVF8tavVEd/6eeeqqbvfnmm26mqgp6yx111FHuMu94xzvc7De/+Y2bNTY2utmRRx7pZmvWrHGz0tJSN9u6daubKapiYtYqgur96WvUuOiO5dR1TY3DrGNUVff8+te/7maqcqCqVO5VklTVGVXFYpWpc0V5eXmm5RoaGtxMvefDhw93M1X9XL0/ra2tbpa1kmQhVbzfXapS5bx589xMVfdU5zi1jzZu3Ohm6nyrjjlVYVW9l+r4UJVZve0sKipyl1GdBDZt2uRm6jyS9Titr693M1UhVlUAV/fdzc3NblZVVeVmar+oeYqHTxYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEj0idYZ3VGqecmSJW6myhA3NTW5mWo94Xn99dfdrKamJtNyqqy3KjWcNUPv0h3jKetyt912m5tdfPHFbqZKQ6uy72qMfupTn+r0cVWi+v7773czr12OmdmBBx6YaTlVuluVOlftgFSbC/XaVWsT9XyLFy92s75mb4+nrF555RU3u+OOO9xMtXIZO3asm6nXp9oOeMf/oYce6i5TW1vrZup8oFpSVFdXu5lqcaBa0axcudLNsrbDUu0d1P2FOh7U+Um1QChUAwcOdLOnnnrKzVT7MtXqRLVPKS4udjN13VbjULXjGD16tJsp6vzujSnV1kdR5wr13qnl1L5Ur021slBtalTbkKytdjZv3uxm559/vpt5+GQRAAAAAJBgsggAAAAASDBZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIBEn2idkdXWrVvdbL/99nOzESNGuNmAAQMyZatXr+70cbWNWUtUZy3Z/+qrr7qZKsuv7O0S8H2NKvGsqP3eHe+JarFwwQUXuNnvfvc7N1PHuCpB/573vMfNDj74YDd78cUXO338t7/9rbuMauExatQoN1OlzlXpblUuWy2nzk1Zy4hv2LDBzVTJfmXFihVupsqW90aqxYJqS6LO74sWLXIzNdbmz5/vZtOnT3cz9RpUyX5VSl61h/HOM3PmzHGXUeX1s17r1f3DvHnz3Kyurs7NVMufadOmuZlqlZV1bKtzr2o30htbZ+y///5u9t73vtfN6uvr3WzYsGFupsZMc3Ozm6njqn9//1ZfteNQ7VrUsaPGqHdPq+511TGsWvCosa3uSdSxr85NavyqY1+N7azHg7pWZsEniwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEgwWQQAAAAAJPpE6wxVwle1AVDL1dTUuJkqy6/K3KoSzN52qu3PWt5XtblQJb+VxsbGTMthz3RHmwtV0l6VY77rrrvc7Nprr3Uz1UZh9OjRbnbhhRe6mRrbS5cudbPbbrvNzbxjXJXuVtS5orS0NNM6lazluVWm9rPKVNsBRW1Lb3Tvvfe6mSpbX11d7WaqfUpDQ4ObqdLuqjWDKt+uWseoca/OM+pas3bt2k4fVyX0VZn/devWuVltba2b/e1vf3Mz1VJGvQdtbW1uptoAqNeuXp/aLxUVFW7mtQEzMxs/frybFSp1vVAtKZQ333zTzUaOHOlm6pyqWgep64m6LqhMjV91DPTr1/lnVOq5VHsP1WZKLZe1XZS6P1JzAzW2m5qa3EyNQ9V+S517s+CTRQAAAABAgskiAAAAACDBZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEn2idUZW69evdzNVyla1Kxg2bJibeWW9zfxyvFu3bnWXUebPn+9mqmTwG2+84WaqXLZ6vqyytkRBzi9+8Qs3u/TSS91s8uTJbrZq1So3U+Xbhw8f7maVlZVuNmPGDDdTY+Pmm292s/b2djdTJcYnTZr0lpdRbQxU2XX12tS4UFTbCdUyR2VZx6h6farUvzq/FqrrrrvOzVQ7BLUfli1b5maqZLp6v1SbmuXLl7vZiBEj3EyNe9XGQx07akx5ZfnVsa9em7pHUOtU7XRUOX91PKh9oloVqOu2KvWvWhKo/aJacxUqdT765je/6WZDhgxxsxdeeMHNTjzxRDdT76W6ZqjxpNq1eGPGTO+X+vr6TOv0qFYc6hyj2r+oFnJZx4w6L5eXl7uZuj9S5zR1jGW9h8jS8oVPFgEAAAAACSaLAAAAAIAEk0UAAAAAQILJIgAAAAAgwWQRAAAAAJBgsggAAAAASPSJ1hlZ2yjMmzfPzQYPHuxmqvVEY2Ojm6mSu97zqRK+iirrrcpeq/Yeap+sXLky07aofYldU+XNb7nlFjdT7SoU1UZBledW7SqGDh3qZo899pib3XfffW6mSlir1gKqDYZXbjprSwo11tRyahyqkuWqXLY6N6my3tu3b8+UqbLe6rWrsvzqvetJzz//vJvNnTvXzVTZ96yZoo4rtW/V+zxo0CA3U20i1GtQx7hXzl8dw83NzW6mXpvaDnVeVmNN7a8JEya4mWqBoa736t5JvXb1Gv70pz+52axZs9ysJ6lz48UXX+xmTz/9tJs999xzbvaVr3zFzVRLB5WNHz/ezdT4VceOakuhjg91TfeOHTUO6+rqMj2Xul6o64wav+qeVb0/qmWOeg3qWqmWU68vCz5ZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIAEk0UAAAAAQILJIgAAAAAgsU+3zpg/f76bqfL6LS0tbnbUUUe52caNG91s6dKlnT4+duxYdxlVTvjYY491syeffNLNDjzwQDdTpc5VWW/VTqSqqsrNYoxulvU972t+/OMfu5k6Pg499FA3U+WyJ02a5GYHHXSQm6ky81lLu6vS0OrYUVSJ/SzPpcqLq1YF3fHaVIl0tU61LarkvFqnWq66utrNnn32WTcbN26cm/Wke+65x81Ue5tly5a5mbp2qTZGqjy9agGjSsKrUvLq2FHHo1qnOpcMGTKk08fVWMvaikZdz7O2I1Al+xcuXOhm6jyTdfx6+9LMrKmpyc3UtaU3mjJliptdeeWVbnbIIYe4mXq/Ghoa3Ezdg3ltY8z0cazO0ypTbV5WrVrlZt41Vo2ZUaNGuVnW+wfVnkctp9pVqPYYqgVGWVmZmy1fvtzNRo4c6WZdfY/MJ4sAAAAAgASTRQAAAABAgskiAAAAACDBZBEAAAAAkGCyCAAAAABIMFkEAAAAACTCLsqwZ6vR3g26o43CRRdd5GaqJO3atWvdrLy83M1USXOPet2qjYFqAaBKDQ8bNszNVAnu2tpaN7vgggvc7IgjjnCzAmudUZC9Otra2tyd9KEPfchd7uGHH3YzVeJZleBWJeiVkpISN+uO9znrceWVvN+wYYO7jCqRrvalora/O0qkZ23VkbVseXt7u5v96U9/crOjjjqqIMeo7eXrqBq/qjWDKtGurl2LFy92M9U2adOmTW6mrrFZWsCoZVSWdfyqthOlpaVuVllZ6Wbq2qzOvaqcf9btVOfsqVOnulko0J5X999/vztGr7nmGne5yZMnu5lqfVNRUeFmixYtcjPVrkVRbXFUCwx1nlbtbVQ2cODATh/P0rbKTG+/OseoFjaqpYxaTjnyyCPdLOt91X/9139ler5dnNc6HaN8sggAAAAASDBZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIAEk0UAAAAAQKLPt85Qy11xxRVupsrVqhYSqvR1U1OTm3nlklVJXVX+dsKECW6mtl+VSlb7cunSpW524IEHutmnPvUpNyswBVny27phjLa2trrZvHnz3EyV0Fel9xsaGtxMlQpXY1SV01al5NV423///Tt9XLXZGTFihJupsabG6N5uj6FaMagS46osvzpnl5WVudku7DNjFOilCnKMrl+/3h2j6r5NtSFT94LNzc1upu7rsrYjytJuZlfPp7ZTterwWrKoa6VqxYHO7UHrOVpnAAAAAAB2D5NFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEgwWQQAAAAAJPy65wUmaxnYVatWuZkqe6yer6Wlxc1UOWFVDn/8+PGdPr5582Z3mZKSEjdTpZJra2vdbNasWW7W2NjoZqNHj3Yz1Rohqz0oC4xdKC8vd7Njjz02UwYAQCFSbSfGjRvX5c+n2moAXaGr74P5ZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgESvaZ2RtQzssmXL3Ey1pdiyZYubqTLLS5cudTPVkmDDhg1v+bkGDhzoZnV1dW6mzJ49282mTZvmZps2bXKzxYsXZ9oWAAAAAD2HTxYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEj0mtYZWb300ktuNmrUKDcrKytzs+bmZjcbNmyYm02aNMnNvPYS/fv7b9HgwYPdbObMmW42d+5cN1P7pLS01M22bt3qZtXV1W4GAAAAoDDxySIAAAAAIMFkEQAAAACQYLIIAAAAAEgwWQQAAAAAJJgsAgAAAAASTBYBAAAAAIkQY1S5DPem7du3u1m/fv6c94wzznCz5557zs1OP/10N2tqanKzqqoqN1u9erWbrVy5stPHZ8yY4S6zZcsWN9u0aVOmbOPGjW6m3oOWlhY3e+WVV9ysvb3dzRR13IYQMq1zF7plpV2gYMYo0MMYo0BhY4wCha3TMconiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEgwWQQAAAAAJHpN64ysVLuHq666ys1UWw3VOkO1pVAtHTZs2NDp44MHD870XKoFhmrvsX79ejcrKipys+rqaje79NJL3WzmzJluVmAo+Q0UNsYoUNgYo0Bho3UGAAAAAGD3MFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgASTRQAAAABAYletMwAAAAAA+yA+WQQAAAAAJJgsAgAAAAASTBYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEgwWQQAAAAAJJgsAgAAAAASTBYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEgwWQQAAAAAJJgsAgAAAAASTBYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEgwWQQAAAAAJJgsAgAAAAASTBZ7SAjhghDC0x3+O4YQDujJbQIAAACyCiEsCSG8w8lODCG8sbe3CXuGyWIXyA+MDSGE9hDCqhDCHSGEkp7eLgDZhBA+GEJ4KT+m60MIj4QQTtjDdf4xhPDRrtpGYF+203V3dQjhoRBCTU9vF9Bb5cfSjn/bO4yv9hDCeV3xHDHGOTHGg3axHZ1ONvPX5XtCCBPyH7D074ptwq4xWew6744xlpjZkWZ2lJld2cPbIzHIgM6FED5tZt82s2vNbJSZjTOz75rZe3twswCkdlx3x5jZKjO7pYe3B+i1YowlO/6Z2TLLj6/8v5929/Pvxn3pGWb2cHdvB1JMFrtYjLHOzB4xs0N2/s3H7n6yEEIoCyH8JITQGEJYGkK4MoTQL4QwKITQGkI4pMPPjsj/9mdk/r/fFUJ4Of9zz4YQDuvws0tCCFeEEF41s3VMGIF/FkIoM7Ovmdn/F2P8VYxxXYxxS4zxgRjjZ/Nj8NshhBX5f98OIQzKLzsshPBgftyuzv//6nx2jZmdaGa35n9Le2vPvUqgb4kxbjSzX5jZNDOzEMKZIYS/hhDWhhBqQwhXdfz5EML5+WtrcwjhS+prcwBSIYTK/DWuNYTQEkKYE0LoOKeYHkJ4NYSwJoTwsxDC4Pxys0IIyzusZ+f70nst9wvaB/LXys/lf66fmZ1qZo+a2VP5xVvzP3Nc/h75yvy4bsjfQ5fll93xSeTH89ft+hDC5d2/l/oOJotdLP81mDPMbPUerOYWMyszs0lmNtPMzjezj8QYN5nZr8zsAx1+9n1m9mSMsSGEcKSZ/djMLjKz4Wb2AzP77Y6b2bwPmNmZZlYeY9y6B9sI9EXHmdlgM/u1k3/RzI41s+lmdriZHW3/+BZBPzO7w8zGW+5it8HMbjUzizF+0czmmNnF+d/SXtxN2w/sc0IIRWb2fjN7Pv/QOstdN8std737PyGEs/I/O81y3xQ4z3KfSJaZ2di9u8VAr3e5mS03sxGW+wbO/zWz2CF/n5m908wmmtlhZnaBWFfH+9IP2D9/qnl9/meONrNFMcYmM3t7/rHy/M88l1//BWZ2kuXunUssf/3t4CQzm2xmp5nZ5/kF0e5jsth1fhNCaDWzp83sSct9he0tCyHsZ7mL3hdijG0xxiVm9i0z+4/8j9xj/zxZ/GD+MTOzj5nZD2KML8QYt8UY/9fMNlnu5naH78QYa2OMG7JsH9DHDTezJvGLlPPM7GsxxoYYY6OZfdXyYzPG2Bxj/GWMcX2Msc3MrrHcL3sAdI8d1921lvvU4ZtmZjHGP8YY58YYt8cYXzWze+0fY/EcM3sgxvh0jHGzmX3Z/vkmF8CubbHcL1vG5799MyfG2HEcfSfGuCLG2GJmD1juF6ye3bkvPdP0V1DPM7MbY4yLYoztZvYFMzt3p2/QfTX/baG5lvvF7gc6WxFSTBa7zlkxxvIY4/gY4yct96lCFpVmNtDMlnZ4bKn94zeffzCzISGEY0II4y03AHd8CjLezC7Pfy2gNX8RrTGzqg7rqs24XcC+oNnMKsVXtKssHZtVZrlPN0IIP8h/DWat5b4qU57/BRCArndWjLHczAaZ2cVm9mQIYXT++jg7/5XwNWb2CctdW81y4/X/XQdjjOstN+4BdCKEMC50KH6Tf/ibZvammT0eQlgUQvj8Tout7PD/11vukz7P7tyX7urvFTu7Nve33KeenT3P/7t2Y9eYLHafdfn/Lerw2OjdWK7Jcr+xGd/hsXFmVmdmFmPcbmY/t9xvRD5oZg/mP8Uwyw2Ea/KT1h3/imKM93ZYF79BBXzPmdlGMzvLyVdYOjZX5P//5WZ2kJkdE2MstX98VSbk/5exB3SD/DdpfmVm28zsBMt92+a3ZlYTYywzs+/bP8ZhvZlV71g2hDDEct8oANCJGOOynYrfWP6bb5fHGCeZ2bvN7NMhhFOyPoX67xDCaMt9ivkX5+fNOr82b7Vc4asdanbKVxh2C5PFbpL/ilqdmX0ohLBfCOFCM9t/N5bbZrnJ4DUhhKH5Tw8/bWZ3d/ixeyz3VdXz7B9fQTUzu93MPpH/rWoIIRTn/9B/aBe9LKBPizGusdzX0m4LIZyV/7RwQAjh9BDC9Zb7OtuVIVdYqjL/szvG5lDLfaOgNYRQYWZf2Wn1qyz3txQAulD+evdeMxtmZq9bbiy2xBg3hhCOttwvVnf4hZm9O4RwfAhhoOW+Sh6SlQJwhVwxxQNCCMFyXwPflv/XFXa+Vp5hZo92+Jpro5lt3+ln7jWzy0IIE0Oudd21Zvaznf6k5Ev5a/rBZvYRM/tZF21vn8dksXt9zMw+a7mvuBxsZs/u5nKXWO6TyUWW+xvIeyxXuMbMzGKML+TzKstVXt3x+Ev557zVcgV23jT9R8UAdhJjvNFyv6C50nIXpVrLfcXtN2b2dTN7ycxeNbO5lvtN59fzi37bzIZY7tsBz1uualtHN5vZOSFXKfU73foigH3DA/mvxa213N8IfzjG+JqZfdLMvhZCaLPcL3R+vmOBfH6Jmd1nuU8Z28yswXJ/3w9g90w2s9+bWbvlvpHz3RjjH7to3f9tuV/KtoYQPmM7fQU1/9Xxa8zsmfzPHGu5e+S7LPfnH4st9w2hS3Za75OWuy9+wsxuiDE+3kXb2+eFf/57VAAAgH1D/lOIVjObHGNc3MObA6CDfP2AlWa2f/6bP1nWMcFyE8gBdAHIhk8WAQDAPiOE8O7819GKzewGy31LYEnPbhWATlSY2ZeyThTRNZgsAgCAfcl7LVfcYoXlvk53buRrVkDBybep+l5Pb8e+jq+hAgAAAAASfLIIAAAAAEgwWQQAAAAAJPrvIt9nv6Oqvp6bayvTuXnz5rnZnXfe6WZFRUWdPn733Xd3+riZ2eOP+1V/J0yY4GbbtvmtcPr1839/oF73PqBQX/w+O0affdbvRPO+973PzQ477DA3W7PG/xv6oUM7b1f65ptvusu0tra6WUVFhZtNmzbNzb7ylZ3bN/7DEUcc4Wb7AMYoUNgYowVm+/btbqbuB5966ik3mz17tptVVVV1+viGDRvcZVpaWtxsypQpbnbuuee6maL2iboP7iP3yJ2+CD5ZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIAEk0UAAAAAQCKoqp/WSypE7eI1uLqjctH111/vZjfddJObHXLIIZ0+Pn/+fHeZn//85252zDHHuFl3yFo9thcp1BfRK8Zod1CV2oqLi91MVSHdtGmTmw0cOLDTx1XltMGDB7uZV13VzGzFihVu1tDQ4GYvvfSSm82YMcPN+gjGKFDYGKO7kLU66d6WtSpo1vv1LPbmc+1Kb3lfjWqoAAAAAIDdxWQRAAAAAJBgsggAAAAASDBZBAAAAAAkmCwCAAAAABJMFgEAAAAAiT7ROmNvW758uZtdcsklbrZ48WI3mzhxYqeP19bWusvU1NS42eWXX+5mRUVFbnbkkUe6WXfoRS03CmpjOtirY3Rvv1/XXHONm91xxx1uNmTIEDcbNGiQmzU3N7uZ99o3btz4lpcxMzv44IPdTLXwUOeR0047zc3uvPNON+sjGKNAYWOM9oBFixa52RNPPOFmX//6191MXdtUy6hJkyZ1+rhqF7V161Y3U1R7qs985jNudvzxx7vZ2LFjM21LL0LrDAAAAADA7mGyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgERBtc7ojrL88+bNc7Ovfe1rbvb3v//dzerr691szJgxbtavnz83f9vb3tbp408//bS7jCpPvGrVKjfbvHmzm6l2HIcccoibnXfeeW72L//yL27Wi1Dy27pnjC5btszNZsyY4WYlJSWZMnUc//znP3czb4x6j5uZtbe3u9lTTz3lZvvtt5+bqfGrXvfcuXMzPV8vwhjdA7/5zW/cbMKECW42evRoN9u2bZubqZYzq1evdjPVVmbgwIGdPr59+3Z3GVWWX41fpaKiws3UOFRtfdQ1dm/bxX2jKxRYP6wOesUY/exnP+tmjz32mJvV1dW52ZYtW9ysuLjYzVRbipaWFjcbPny4m3nUPffUqVPdTN1bbNiwwc3UvbW6Rz7llFPc7LrrrnOzAkPrDAAAAADA7mGyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgERBtc7I6kc/+pGbXXHFFW6mylSXlpa6mWqBoUp0r1271s1OPfXUTh+fM2eOu0xZWZmbqdc2YMAAN1OWLFniZt72m5nddNNNbqb2c3e0adgDlPzeA//zP//jZur4OPzww91Mled++eWX3UyVCletOsaPH9/p4ytXrnSXUWNejaejjz7azVRZ7/Xr17vZo48+6mZvvvmmm6l2QIzR3eLuJHV8qGuJanWiStorlZWVbvbOd77TzV555RU3a2trczPVckZdF1SrixEjRnT6+JAhQ9xlVNa/f/9M26Heg0WLFrmZavWlxm8v0uvGaHdQbRtmzZrlZq+99pqbece+mVlRUZGbqfOMOv7VtUa1vvHG28SJE91lVCs7dd5SrT/UtUu1/FGtexobG93siCOOcDM1tr12QN2I1hkAAAAAgN3DZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAACJXtM6o7293c0ee+wxN7vzzjvdrL6+3s1UyV1VFliV/K6rq3OzM888s9PH//rXv7rLqJK6qq2GKnmctS3Ieeed52aqTPRHPvIRN1PtP3oAJb/3wFlnneVmqn3AhAkT3EwdjypbsGCBm6m2GjU1NZ0+rkp+qzGvSu+/5z3vcbMXXnjBzdS+rK2tdbNx48a52V133eVmBaYgx+irr77qjtEbbrjBXU61LPnLX/7iZmeccYabnX322W528803u9m3vvUtN1Ol5NetW+dma9ascTPVGkQ9X2tra6ePq3ZRWdtMqWulWqcq9X/77be7WUlJiZupcT979mw3q6qqcjN1D6Ta4nzjG99wMyvQMWp7+Tr6wQ9+0M3U/ax3DTLL3oZHvZcqUy2cFG/OUV5e7i6j2mSp9h5Z7xHU2FaZGqOqPdWBBx7oZs8995ybdRNaZwAAAAAAdg+TRQAAAABAgskiAAAAACDBZBEAAAAAkGCyCAAAAABIMFkEAAAAACT8mrMF5o033nCzYcOGudlBBx3kZosWLXKziooKN1Olejdu3Ohmqhy4t05V3le1E1HluYuKitxs5cqVbqa2f9KkSW728ssvu1ljY6ObVVdXuxkKjyqTP3/+fDcbO3asm6k2Nap0t2qBMXnyZDdTx7/XXuJd73qXu8xXv/pVN1Nl0B944AE3U+X8VSskVQrfazmAPffFL37RzWbOnOlmqmz9BRdc4GaqXdQvf/lLN1Ml4Y844gg3y3JdM9OtYzZv3uxm6jhW2+JRraRU2yd1bVavW92vjBgxws2GDBmS6flUiy31GtR7oK7b6tyrzl19zYoVK9xMtcdQ927qPK3e523btrmZos5BqoWNug55bWXU+Ucdi6pNjVpnVmpfNjc3u5m6z1m6dKmbqbGmzhddjU8WAQAAAAAJJosAAAAAgASTRQAAAABAgskiAAAAACDBZBEAAAAAkGCyCAAAAABI9JrWGWvWrHEzVcJ3ypQpbvbDH/7QzVT5YkWVsl27dq2beaWGVelfVTK4paXFzUpKSjKts6qqys1UyXJVIl21RqB1Ru+iWtGoMarKvqvjSpX1VsupbVHHqlf6evjw4e4yahxOmDDBzbKuM+v5Yvny5W5WKKW7e6urrrrKzW677TY3mzZtmpupFk3HH3+8m9XX17uZKk+vyvKr5dS1Ro1Rdb1X2+K1hcra5kK9NlVCX401RbXqUC031HLqGqv2izpPqrZF3dGuoDdSbZOy7nfVlkQdxypT61SyrlOdu7JQz6X2pTpOs7TgMcveakdl3/jGN9zshhtu2L0N6wKMagAAAABAgskiAAAAACDBZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEr2mdYYqgatKaavlVNl31SZClfVWJbOnTp3qZnV1dZ0+rkr4qlLaS5cudbMVK1a4WVlZmZspy5Ytc7OKigo3a29vz/R8KDyzZ892M1VmXh0769atczPVWkC1nlDtMdR2emWxm5qa3GWytvdQ2+i1BzDT558FCxa4mSqv/9JLL7nZ6aef7mbIUe+Xuj6pMuzquBo6dKibqeO7ra3NzbzWTmZmI0eOdDN1HK9cudLNVFn70tLSt/x86nWr51LtAVQLgKwtbFRLCnU8dMd7rtqeqHO2er6sLQl6o7vuusvNVGs21UZBvV/qPlitM2urE7WcOj9526KOG5Wp47SmpsbN1q9f72bqvKXGvboXyNLyx0y3+KN1BgAAAACgRzFZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIAEk0UAAAAAQKLXtM5QJWlHjx7tZi0tLW6myj+fdtppbtYdJXe9ksiTJk1yl1GltNU+UaW7N2/e7GZ/+tOf3OzDH/6wmw0YMMDN9qVS2n3dn//8ZzdTx5waM5MnT3YzVc5flRhX5enVtnilr1X7A7U+1RZErVO9bvXaVDud4uJiN/v1r3/tZrTO2DXVIkIdH6tWrXIz9T6rNgrqGpS1lUvW9keq5L2yZcsWN/POM+r8k7W9jVqnKpOv2nEoqv2Ban2j2mGpbVHnoKznPLVfeiN1nVHjUN0TqTGq9rs6J6hrjdoWRR2PKvOOgfLycncZde1S9+Nq/Kp9mXXcr1271s3U8aBaZ6hWQVmPvyz4ZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgESvaZ2hysCOHTvWzWbOnOlmqsSzKneuSoWr0rmqbLRXxvf11193l1GlklXp39raWjdT+1K1MVDluefOnetm9fX1bnb00Ue7GQrPK6+84mbq2FdtcVRpaFX2XbUdGD9+fKbn884X27Ztc5dRbS7Uco2NjW6mSoWrNgDqfKfOF3PmzHEz7Nqzzz7rZurYr6ysdDNVml61P1L69fN/d6zGmjrm1DVKlexX7THU2PCuzer8o/alet1q+7O2nVDnhCFDhriZeu/U9V69PrXOESNGuNm+5LHHHnMz1dps+PDhbqauGeoYyNoKQh1zamwr6vm8sd3Q0OAuk7X1llpnlvYeZnrMqHmKWmfWVipPPvmkm73rXe9ysyz4ZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgESvaZ1x9913u9nBBx/sZqqs7rhx4zJty/Tp091MldUdPXr0W15ObX9FRYWbqbLXWVsVfO9733OzL3/5y26myqf/5S9/cbOvfOUrbobCs3DhQjc74IAD3Ewd4+vWrcu0nCq1rcryqzLiXqltVYK7q5/LTJclV69blXJXpbvr6urcDHtmzJgxbpa1bL1aLmsrCNWOQ5WEV9cTVepflaBX123vnKD2yerVq90sa1sNRV2b1RhV58KioqJMz6fOQcOGDXOz5cuXu9maNWvcTG1nb/TEE0+4WXl5uZupdiaqdYY6BtTxqFp1qHWqc4LaTpV594PqWqlem3outf3q2Ff3rM3NzW6mziXq9anrdklJiZs9/PDDbkbrDAAAAABAt2OyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAAAJJosAAAAAgERBtc5QpWVnz57tZqoEd0tLi5up0rmf+cxn3EyVuc1aRtzbFlUmX5UzV6V/jznmGDdrb293sx/84AeZllNlj9X+ylquHd1HlalW40kdAxs3bnQzVaZalQNXZd+ztiTwytqr0uNqfylqOXVOUO/BkCFD3EyNJ9WiRLUWUCX79yXHH3+8my1evNjN1HVGXfPU8a2Onayl8NV5OmtZe3WtVMt5WWVlpbtMWVmZm2W9t1AtMJTi4mI3U/dHqpWFOveqc4Jqc1FaWpppub5GtRNT1Pt1//33u5lqNfbkk0+6mbrGquNDnUuytn7yxlTWc5raDnXtUvfI6n5WXStPPfXUTNm5557rZlVVVW6mzuddjas5AAAAACDBZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAACJgmqdocpNqzK3JSUlbqbK8aryuKosdnl5uZs1NDS42Zo1a9zMK7+vyoQPHTrUzRRValjtE7X96v1RpYbV61PbUlFR4WboPqqkvSqTr9ooqPLPU6ZMcTM1RtWxqloSqFLb3utTJe1VCXG1v9TYVue0uro6N1MtDtR2Kq2trW7GGM1R7V/UeVOVi1fHnDq+FfV86hyetSy/uo6q40qdS7x7iNdff91dRrV6yPJcZrqdjjrfqdetzgmq5YY6Z6ttWb9+vZup40+9BtWmZF9SXV3tZpdddlmmdZ5//vlu9vDDD7uZakGlrhnqfJE162rq+FZjW11jVQuMO++8c7e2qzfik0UAAAAAQILJIgAAAAAgwWQRAAAAAJBgsggAAAAASDBZBAAAAAAkCqoaqqpgOGTIEDdTlfxKS0vd7KSTTsq0TlUlTFWPqqysdLMxY8Z0+riqCKqquKmqiKoK4+rVq93s8ssvd7PHH3/czVRFPPX+qCqCVFrsGStWrHAzVeVMjSdVlezkk092s9tvv93Nampq3ExV4FW85bJWflNViVWmxow6/6j3Tp0TlPr6ejdjjOaoCruKuuapCqRZq2QrWSv+qtegxv2SJUvcbMGCBW7mVQVV1UnVvjzooIPcbOrUqW6WtdKiot6DrOcLVZFW3eeoew91vd+XqHNq1kxV91RjLaus1+1CkfVYVMupjgdZqf2ctXpsV1edLfx3GwAAAACw1zFZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIAEk0UAAAAAQKKgahyrVgmqZLAqG71o0SI3+8lPfuJmjzzyiJtNnDjRzVTZclX62ivHO2zYMHeZgQMHutm4cePcTLUOUK06VBuDJ554ws1UOw6VqVYq6BlNTU1upsaoOlbVuDjzzDPd7Pvf/76bqTLiWcuWe+NGbb8qL67OW4MGDXIz5dhjj3WzV1991c1UawFVglsdD8hRx5Taf2VlZW6m2j1kPa7Udqr2H+oYV8eVOieo4/iMM85ws9ra2k4fV8dw1hYRajl1HVXnC3X/oN47tS9VG4DBgwe72YYNG9zM289m2dvw9DVZWyqpNgrK8ccf72b3339/pnUWiqz7K2vrjKz3CFn1hjYkhb+FAAAAAIC9jskiAAAAACDBZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEgXVOkOVos5aSn748OGZllOlbGtqatxs+fLlbtba2upmXrlzVbJclV1Xy6ntVyXZVSn30tJSN1Pv69FHH+1mWUtIo/u0tLS4mSpTXVdX52bTpk1zswMOOMDNVHl61R5GHVeq7YBXvluVmFdUqxHVckCV7lbnSVUmX50v1DppnbFrhx9+uJv9/ve/d7OxY8e6mbrOeG2YzPSxM3LkSDdTx446jlX7I3VdUFlxcbGbrVq1ys08o0aNcjPVbkZdD9W5sLKy0s3U9Ved71TLDfWeb9682c3UNV2dzysqKtwMu6baRChVVVVupu5n1fjtjhYSWZZT26iu5+r4Vu211POpMdqX8ckiAAAAACDBZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAACJgmqdoUrSTp482c1Umdvq6mo3U6WoVQluVV5flfFVpcm9dar1qdLdqsWBKsuvyjarNiSNjY2Znk+95/X19W6GnrFy5Uo3UyXt165d62ZnnHGGm/3sZz9zMzXut2zZ4mYlJSVuNnDgwLecZS0hrsqZZy2fvmzZMjdT+0Q9n9ontbW1u7dh+zDVGmbJkiVuptpVqHYPqiR81nOxaq0yadIkN1Pb2dbW5mbNzc2ZsqlTp3b6uLpmL1y40M2mTJniZmr8qte2adMmN1OtRtT5VY1t9XyqjceCBQvc7JprrnGzoqIiN8OuqeuJOk+r4zHr8/UGap+oca8ytU9UW5y+jE8WAQAAAAAJJosAAAAAgASTRQAAAABAgskiAAAAACDBZBEAAAAAkGCyCAAAAABIFFQN2D//+c9u1tra6maqhL5qubF582Y3U2V1VUlzlanS1+vXr+/0cVXC11tmT7Zj6NChbqbaeJxyyilutnjxYjcbO3asm6nXgJ6hWjMMGDDAzVSbl3vuucfN1DnhwAMPdDN1TlAtAtSYGjRo0Ft+rqxUGXT1fKq1iTrfqUy9r6pFD3JU25g//OEPbjZixAg3U+0jxo8f72bqHL506VI3U+Xim5qa3Ey13FAtWVS7B/XavbGhriXq2Ff7RLUhUa9NPZ8a22pfNjQ0uJm6l1HHWF1dnZup9kPoGeo8nbXdQ9YWToVCne8U9bppnQEAAAAAQB6TRQAAAABAgskiAAAAACDBZBEAAAAAkGCyCAAAAABIMFkEAAAAACQKqgbsoYce6maq9LUq3f3Rj37UzRYsWOBmK1asyJSpNhGqHLjXIqOoqMhdRpXnViW4hw0blmmdc+fOdbNZs2a5mSpf/G//9m9uNm7cODdDz1izZo2bZS37rkq7l5aWupk6rrKWvs5SFluNa/W6ValztU9UNmrUqEyZahlSXl7uZlu2bHEz5KjrzIUXXuhmkyZNcrO2trZM2zJy5MhMy6ljVbUGUVl7e7ubeW1qzPS9wIYNGzp9vKtbcZiZVVZWupk6/6h1qlYWqn2YapeiWgWtXr3azSZOnOhman+iZ6h7RdWKKWt7iUKhxpp63aotndon+2rbGD5ZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIAEk0UAAAAAQILJIgAAAAAgUVCtMyZMmOBmZWVlbvbKK6+4mSpF/dRTT7mZKluutlO1pVDleNetW9fp46pMryp7nbUdgcrmzJnjZtOnT3czVbpblTZWr6+qqsrN0H1UawlVEt4raW9mVlxcnClTZfnVtihq3HivXbWdUPtLHftZt1+tU7XOqK2tdTNVYnzz5s27t2H7sL///e9uplqPqFL46tyojjn1fKrNi1rn0KFD3UxdT9Ryimob4h3/6jyi7i3UNVuNC9VaQmXqWqnGmjpfqHWqNiRqO//2t7+52dlnn+1m6D7qOFbXNUW1pVBZV8va3kO1/FHrVOfCioqKTNvS2/HJIgAAAAAgwWQRAAAAAJBgsggAAAAASDBZBAAAAAAkmCwCAAAAABJMFgEAAAAAiYJqndHU1ORmqj3GwIED3WzQoEFuVl9f72aq1P/ixYvdrL293c1UKXTv+VSZcEWVClfrVOWQV65c6WZjxoxxM68tiJku6ayWQ89QY02VWs/ackOVsFbHjtoWpbKy0s28saHaEajWAapdhSrrrcavOoeq8506T6ptUa8dOerYV201jj76aDdTY0a1dFCl5FUbBXVdUOcEdQ5vbGzM9HyqPYx3rLa0tLzlZcz0flbHvmpzocaaagek3jvVHkMtp6hzLwqPOq4Udb7o7dQxrPaXurdQ9/h9GZ8sAgAAAAASTBYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQKKjWGarc/fjx491s1apVbqZKcKuyujU1NW42dOhQN8ta1n7kyJGdPq5KYqv2GOp1q9K/qpzwwoUL3ay5udnN1HaqsvKU5S886v1SJehVuXtVLr6urs7N3njjDTdT5cAPO+wwN1Ol/hcsWPCWl5k3b56bHXHEEW6mWvesX7/ezaZPn+5map+oc5Nq+ZO1Rcm+5MADD3SzI4880s1UqxN17VLnTdUeo6yszM3UsaOuGSpTx7g6XyheyfusbQXUdqhzYdZ2BKpVh6KOB/UaVIsP1T6gqqpq9zYMe406T2cdT71B1tem7lfU/bM6L/dlffcIAgAAAABkxmQRAAAAAJBgsggAAAAASDBZBAAAAAAkmCwCAAAAABJMFgEAAAAAiYJqnTFkyBA3U+XAVel9RZWpVqVzVclsVcZ37Nixbua1nlAlsVUZdNUeo6SkxM1UyxBVJt9r/WFmNmDAADdTrUGGDx/uZugZqtx9ZWWlm6nWDLNmzXKzX//617u1XV1l8eLFbuaN34EDB7rLXHXVVW5WXV2929vVFa6++mo3u/nmm91MlWTP2iJgX6Jaqzz00ENuNnPmTDdT17zy8nI3Uy2O1Pk9a3l6tU51XVDXNsU7P6nnUtfzrFRLClWyP2trIvX6Ghoa3Ey1JGttbXWzrK1I0H264zhW1D2y0tXXDDUusrbzUpk6n2el9knW/dzV+GQRAAAAAJBgsggAAAAASDBZBAAAAAAkmCwCAAAAABJMFgEAAAAACSaLAAAAAIBEQbXOUKV/29ra3Ey1l1CWL1/uZlOnTnUzVVZXtaUYOnSom3mlwhsbG91lVDsRVdpabaMqwa1aI0yePNnNVGuBlpYWNxsxYoSboWeokvCq1Ykq9a/acextqtS2Oo49qoXN3qbGmjonqHMJrTN27e1vf7ubPfPMM25WWlrqZgsWLHAzdQyrVgnqOqrWqc4JqjWSGk/qeFQtr7zzTNa2IMXFxW6mWn2pdap9qV7btm3b3EydQ9V9gnoN69evd7NJkya5GfZMd7RKUOdpdVyp5bK2pejq16e2Ud3PZm2dp84JWRVKewyFTxYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEgUVOsMVeZWlYYuLy/P9HzDhg1zszVr1rjZvHnz3EyVIZ47d66bVVVVuVmW51LlhJuamtzsxRdfdDNVdl2ZMmWKm7W2tmZaJ3rGypUr3UwdVyqbMWNGpm1Rx78qi62ocZ+FaiuQVdaS5SeccIKb/ehHP3Iz1fJHtUvBrk2YMCHTcsccc0zXbkgfMWbMmJ7eBGCPZG2tos79qv2RylRLB9V2patlbS2RtbWT2pcVFRWZ1tnb8ckiAAAAACDBZBEAAAAAkGCyCAAAAABIMFkEAAAAACSYLAIAAAAAEkwWAQAAAACJgmqd0b+/vznLli1zs6zlss8++2w3e+GFF9xMtbnYunWrm40cOdLNvNYgqj3AwIED3UyVu1flhKurq92stLTUzZT6+no3mz9/vpupEsWqHQe6z6c+9Sk3u+WWW9zsqKOOcrMvfelLmbZFlbfOatOmTV26PlWWfG877bTT3Oz973+/m6lS7v/6r/+6R9sEANhz6trV0NDgZuq+Tp37Fy5cuHsbVqBGjx7tZqqFltqXfRmfLAIAAAAAEkwWAQAAAAAJJosAAAAAgASTRQAAAABAgskiAAAAACDBZBEAAAAAkCio1hmDBw92s+LiYjdTLSmUQw89NFOGt+bf//3f3WzJkiVuNnXq1G7YGuyJt7/97Zmy7hBC6PJ1Zm0P42lpaenS9Zllbxmi2uncfvvtWTcHANBFsl7XSkpK3Oycc85xs8MOO8zNVOs2tZ2qPZuXqevafvvt52aKav2h5huLFi1yM7W/+jI+WQQAAAAAJJgsAgAAAAASTBYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIBFUiVsAAAAAwL6JTxYBAAAAAAkmiwAAAACABJNFAAAAAECCySIAAAAAIMFkEQAAAACQYLIIAAAAAEj8/x9dZoC0w77JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i in range(8):\n",
    "    index = np.random.randint(0, X_train.shape[0], 1)[0]\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(X_train[index], cmap='Greys')\n",
    "    plt.title(classes[y_train[index]])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'aller plus loin: quelles méthodes pourriez-vous utiliser pour effectuer une telle tâche de classification ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> C'est un problème multi-classe, nous utiliserons donc un modèle de Deep Learning avec un fonction d'activation softmax sur la couche de sortie qui sera composée de 10 unités car nous avons 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "La première méthode que vous allez essayer consiste à utiliser des réseaux de neurones. \n",
    "\n",
    "La première étape est la préparation des données : remise à l'échelle des données, préparation des labels.\n",
    "\n",
    "Astuce : vous pouvez utiliser la fonction Keras **`to_categorical`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# Mise à l'échelle des données, \n",
    "# on remet tout sur l'intervalle 0->1 en divisant par la valeur max\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "# Transformation des labels reponse en categories\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "# Applatir les données pour repasser de représentations 3D à 2D\n",
    "X_train=X_train.reshape(X_train.shape[0], np.prod(X_train.shape[1:]))\n",
    "X_test=X_test.reshape(X_test.shape[0], np.prod(X_test.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Étape suivante** :  création de modèles avec Keras. \n",
    "\n",
    "Construisez votre architecture de réseau neuronal. \n",
    "\n",
    "Dans un premier temps, essayez une architecture légère : pas plus de 2 couches cachées, avec environ 10 unités par couche. \n",
    "\n",
    "Mettez ce modèle dans une fonction afin de pouvoir le réutiliser plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_dim):\n",
    "    # Nous créons un modèle dit séquentiel\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Ajouter le premier calque \"Dense\" de 10 unités, \n",
    "#     et donner la dimension d'entrée (ici 28*28)\n",
    "    model.add(tf.keras.layers.Dense(128, input_dim=input_dim, \n",
    "                                    activation='relu'))\n",
    "    # Ajouter le deuxième calque \"Dense\" de 10 unités\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "    # Ajouter enfin la couche de sortie avec une unité: le résultat prédit\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # renvoie le modèle créé\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, compilez et entraînez votre modèle sur vos données d'entraînement. \n",
    "\n",
    "Puisqu'il s'agit d'une classification multiclasse, la perte n'est plus **`binary_crossentropy`**, mais **`categorical_crossentropy`**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creation du modèle\n",
    "my_model = model(input_dim=X_train.shape[1])\n",
    "# Compilation :\n",
    "my_model.compile(optimizer='adam',\n",
    "                 loss=\"categorical_crossentropy\", \n",
    "                 metrics=[\"accuracy\",  tf.keras.metrics.Recall()])\n",
    "\n",
    "my_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "            epochs=30, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
